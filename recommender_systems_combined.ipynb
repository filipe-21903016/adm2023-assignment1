{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12d3b94-0237-4d10-a8e4-8300262078f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb565d6",
   "metadata": {},
   "source": [
    "Recommender systems are critical in providing personalized proposals for users across various fields. In this assignment, we aim to evaluate the performance of three algorithms covered in the course, namely Naive methods, UV matrix decomposition, and matrix factorization, and to provide insights into their effectiveness using the MovieLens 1M data set. Additionally, we used 5-fold cross-validation to increase the reliability of our recommender systems' results for data (movie or user) that did not occur in the training process. After implementing each of the algorithms, we used the Root Mean Squared Error (RMSE) and the Mean Absolute Error (MAE) over both the training and the test set for the examination of their accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95755b3d",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde23472-3077-4e20-8f80-bb10e31a4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.lines as mlines\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c231150-0ce3-419a-8f86-a9a1f9cc5be0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a0dd82-388f-47e8-a9e3-34a76496e7e0",
   "metadata": {},
   "source": [
    "It appears that there is inconsistency in the text encoding used in various data files. As a result, we must verify the encoding to ensure accurate data reading from these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd895ea-9558-4625-b348-753c6276ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_encoding(file_path):\n",
    "    \"\"\"\n",
    "    This function checks the text enconding used in a particular file\n",
    "    \n",
    "    :param file_path: The file path you wish to examine for its encoding\n",
    "    :return: String containing enconding type\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        return result['encoding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf48e43c-ed09-4306-b8fd-acaf23c9bf34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  Timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading ratings data\n",
    "ratings_path =\"./ratings.dat\"\n",
    "ratings = pd.read_csv(ratings_path, delimiter=\"::\", header=None, engine='python', encoding=get_file_encoding(ratings_path))\n",
    "ratings = ratings.rename(columns={0: \"UserID\", 1: \"MovieID\", 2: \"Rating\", 3:\"Timestamp\"}) # Set ratings column names\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "961137de-33e8-4406-8382-557f1917216b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading movies data\n",
    "movies_path = \"./movies.dat\"\n",
    "movies = pd.read_csv(movies_path, delimiter=\"::\", header=None, engine='python', encoding= get_file_encoding(movies_path))\n",
    "movies = movies.rename(columns={0: \"MovieID\", 1: \"Title\", 2: \"Genres\"})\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4760af66-ca6c-4ff8-af76-292c7d6ab212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  Occupation Zip-code\n",
       "0       1      F    1          10    48067\n",
       "1       2      M   56          16    70072\n",
       "2       3      M   25          15    55117\n",
       "3       4      M   45           7    02460\n",
       "4       5      M   25          20    55455"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading users data\n",
    "users_path = \"./users.dat\"\n",
    "users = pd.read_csv(users_path, delimiter=\"::\", header=None, engine='python', encoding= get_file_encoding(users_path))\n",
    "users = users.rename(columns={0: \"UserID\", 1: \"Gender\", 2: \"Age\", 3: \"Occupation\", 4: \"Zip-code\"})\n",
    "\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d18dcb",
   "metadata": {},
   "source": [
    "#### Missing values check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80edab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(ratings.isna().sum().sum())\n",
    "print(movies.isna().sum().sum())\n",
    "print(users.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd0ef9",
   "metadata": {},
   "source": [
    "# Naive Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176476de",
   "metadata": {},
   "source": [
    "To begin with the first recommender systems algorithm, we implement four functions for each naive recommender approach, namely the Global Average, the Movie Average, the User Average, and the Linear combination (including the $\\gamma$ parameter). The first one, the Global Average approach, involves recommending the global average rating to all users. When movie or user average ratings were unavailable for the Movie Average or User Average approach, this approach was utilized as a fallback value. Proceeding to these approaches, recommendations were based on the average rating received by a movie or given by a user, respectively. Finally, the last approach we implemented was the Linear combination of the three averages. In this approach, predictions are a combination of user and movie average ratings, with the $\\gamma$ term included. In that case, we used the Movie and User Average Ratings in the Linear Combination function. Thus, the fall-back value used for these approaches was used indirectly for the fourth one when user or movie average ratings were unavailable. Hence, in the last approach, the global average rating was implicitly used again as the fall-back value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b663c3e",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb5a5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30005d8f",
   "metadata": {},
   "source": [
    "#### 1. Global Average Rating:\n",
    "\n",
    "$$ {R}_{global} (User, Movie) = mean(\\text{all ratings})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "559be5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_average(train, test, is_train=False):\n",
    "    if is_train:\n",
    "        return [train['Rating'].mean()] * len(train)\n",
    "    else:\n",
    "        return [train['Rating'].mean()] * len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567c852",
   "metadata": {},
   "source": [
    "#### 2. Movie Average: \n",
    "\n",
    "$$ {R}_{movie} (User, Movie) = mean(\\text{all ratings for movie})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0054cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_average(train, test, is_train=False):\n",
    "    if is_train:\n",
    "        movie_avg_train = train.groupby('MovieID')['Rating'].mean()\n",
    "        return movie_avg_train[train['MovieID']].to_numpy()    # A NumPy ndarray representing the values in this Series\n",
    "\n",
    "    else:\n",
    "        movie_avg_predictions = test['MovieID'].map(train.groupby('MovieID')['Rating'].mean())  # movie average predictions for a test set based on \n",
    "        movie_avg_predictions.fillna(train['Rating'].mean(), inplace=True)                       # the movie average ratings in the training set             \n",
    "        return movie_avg_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dcfbb6",
   "metadata": {},
   "source": [
    "#### 3. User Average:\n",
    "\n",
    "$$ {R}_{user} (User, Movie) = mean(\\text{all ratings for User})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ae54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_average(train, test, is_train=False):\n",
    "    if is_train:\n",
    "        user_avg_train = train.groupby('UserID')['Rating'].mean()\n",
    "        return user_avg_train[train['UserID']].to_numpy()    # A NumPy ndarray representing the values in this Series\n",
    "\n",
    "    else:\n",
    "        user_avg_predictions = test['UserID'].map(train.groupby('UserID')['Rating'].mean()) # user average predictions for a test set based on \n",
    "        user_avg_predictions.fillna(train['Rating'].mean(), inplace=True)                       # the user average ratings in the training set\n",
    "        return user_avg_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae6277",
   "metadata": {},
   "source": [
    "#### 4. Linear Combination of the three averages:\n",
    "\n",
    "$$ {R}_{user-movie} (User, Movie) = \\alpha * {R}_{user} (User, Movie) + \\beta * {R}_{movie} (User, Movie) + \\gamma$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95ad6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_combination(train, test, is_train=False):\n",
    "    user_avg = train.groupby('UserID')['Rating'].mean()\n",
    "    movie_avg = train.groupby('MovieID')['Rating'].mean()\n",
    "\n",
    "    A = np.vstack([user_avg[train['UserID']], movie_avg[train['MovieID']], np.ones(len(train))]).T\n",
    "    b = train['Rating']\n",
    "\n",
    "    alpha, beta, gamma = np.linalg.lstsq(A, b, rcond=None)[0]     # https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html\n",
    "\n",
    "    if is_train: \n",
    "        prediction = alpha * user_average(train, test, is_train=True) + beta * movie_average(train, test, is_train=True) + gamma\n",
    "    else:\n",
    "        prediction = alpha * user_average(train, test) + beta * movie_average(train, test) + gamma\n",
    "\n",
    "    prediction = np.clip(prediction, 1, 5)\n",
    "\n",
    "    return prediction\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e35e6d",
   "metadata": {},
   "source": [
    "## 5-fold Cross-Validation \\& Accuracy estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53736f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)   # 42: random seed set at the beginning\n",
    "\n",
    "# training set\n",
    "rmse_global_train = []\n",
    "mae_global_train = []\n",
    "\n",
    "rmse_user_train = []\n",
    "mae_user_train = []\n",
    "\n",
    "rmse_movie_train = []\n",
    "mae_movie_train = []\n",
    "\n",
    "rmse_combination_train = []\n",
    "mae_combination_train = []\n",
    "\n",
    "# test set\n",
    "rmse_global_test = []\n",
    "mae_global_test = []\n",
    "\n",
    "rmse_user_test = []\n",
    "mae_user_test = []\n",
    "\n",
    "rmse_movie_test = []\n",
    "mae_movie_test = []\n",
    "\n",
    "rmse_combination_test = []\n",
    "mae_combination_test = []\n",
    "\n",
    "for train_index, test_index in kf.split(ratings):\n",
    "    train_data, test_data = ratings.iloc[train_index], ratings.iloc[test_index]\n",
    "\n",
    "    # Compute RMSE and MAE over training set\n",
    "    rmse_global_train.append(np.sqrt(mean_squared_error(train_data['Rating'], global_average(train_data, test_data, is_train=True))))\n",
    "    mae_global_train.append(mean_absolute_error(train_data['Rating'], global_average(train_data, test_data, is_train=True)))\n",
    "\n",
    "    rmse_user_train.append(np.sqrt(mean_squared_error(train_data['Rating'], user_average(train_data, test_data, is_train=True))))\n",
    "    mae_user_train.append(mean_absolute_error(train_data['Rating'], user_average(train_data, test_data, is_train=True)))\n",
    "\n",
    "    rmse_movie_train.append(np.sqrt(mean_squared_error(train_data['Rating'], movie_average(train_data, test_data, is_train=True))))\n",
    "    mae_movie_train.append(mean_absolute_error(train_data['Rating'], movie_average(train_data, test_data, is_train=True)))\n",
    "\n",
    "    rmse_combination_train.append(np.sqrt(mean_squared_error(train_data['Rating'], linear_combination(train_data, test_data, is_train=True))))\n",
    "    mae_combination_train.append(mean_absolute_error(train_data['Rating'], linear_combination(train_data, test_data, is_train=True)))\n",
    "\n",
    "    # Compute RMSE and MAE test set\n",
    "    rmse_global_test.append(np.sqrt(mean_squared_error(test_data['Rating'], global_average(train_data, test_data))))\n",
    "    mae_global_test.append(mean_absolute_error(test_data['Rating'], global_average(train_data, test_data)))\n",
    "\n",
    "    rmse_user_test.append(np.sqrt(mean_squared_error(test_data['Rating'], user_average(train_data, test_data))))\n",
    "    mae_user_test.append(mean_absolute_error(test_data['Rating'], user_average(train_data, test_data)))\n",
    "\n",
    "    rmse_movie_test.append(np.sqrt(mean_squared_error(test_data['Rating'], movie_average(train_data, test_data))))\n",
    "    mae_movie_test.append(mean_absolute_error(test_data['Rating'], movie_average(train_data, test_data)))\n",
    "\n",
    "    rmse_combination_test.append(np.sqrt(mean_squared_error(test_data['Rating'], linear_combination(train_data, test_data))))\n",
    "    mae_combination_test.append(mean_absolute_error(test_data['Rating'], linear_combination(train_data, test_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad525546",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = {\"Metric\": [\"RMSE\", \"MAE\"],\n",
    "          \"Global Average\": [np.mean(rmse_global_train).round(3), np.mean(mae_global_train).round(3)],\n",
    "          \"User Average\": [np.mean(rmse_user_train).round(3), np.mean(mae_user_train).round(3)],\n",
    "          \"Movie Average\": [np.mean(rmse_movie_train).round(3), np.mean(mae_movie_train).round(3)],\n",
    "          \"Linear Combination\": [np.mean(rmse_combination_train).round(3),np.mean(mae_combination_train).round(3)]}\n",
    "df_train = pd.DataFrame(data=d_train)\n",
    "\n",
    "d_test = {\"Metric\": [\"RMSE\", \"MAE\"],\n",
    "          \"Global Average\": [np.mean(rmse_global_test).round(3), np.mean(mae_global_test).round(3)],\n",
    "          \"User Average\": [np.mean(rmse_user_test).round(3), np.mean(mae_user_test).round(3)],\n",
    "          \"Movie Average\": [np.mean(rmse_movie_test).round(3), np.mean(mae_movie_test).round(3)],\n",
    "          \"Linear Combination\": [np.mean(rmse_combination_test).round(3),np.mean(mae_combination_test).round(3)]}\n",
    "df_test = pd.DataFrame(data=d_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d71f9cd",
   "metadata": {},
   "source": [
    "#### RMSE and MAE table over training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c03066fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Global Average</th>\n",
       "      <th>User Average</th>\n",
       "      <th>Movie Average</th>\n",
       "      <th>Linear Combination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RMSE</td>\n",
       "      <td>1.117</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAE</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Metric  Global Average  User Average  Movie Average  Linear Combination\n",
       "0   RMSE           1.117         1.028          0.974               0.915\n",
       "1    MAE           0.934         0.823          0.778               0.725"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74ec58",
   "metadata": {},
   "source": [
    "#### RMSE and MAE table over test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c379921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Global Average</th>\n",
       "      <th>User Average</th>\n",
       "      <th>Movie Average</th>\n",
       "      <th>Linear Combination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RMSE</td>\n",
       "      <td>1.117</td>\n",
       "      <td>1.035</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAE</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Metric  Global Average  User Average  Movie Average  Linear Combination\n",
       "0   RMSE           1.117         1.035          0.979               0.924\n",
       "1    MAE           0.934         0.829          0.782               0.732"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91093190",
   "metadata": {},
   "source": [
    "#### Discussion on RMSE - MAE tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572603c9",
   "metadata": {},
   "source": [
    "We compute the RMSE and MAE over both the training and test sets to assess how well the model performs during training (learning) and when applied to unknown data (testing). Consequently, in both sets, the Linear Combination approach is the most accurate naïve method, implying that considering both user and movie average ratings, with the parameter $\\gamma$ in the linear regression model, leads to better recommendations. On the contrary, the Global Average approach is less accurate but still provides valid and straightforward recommendations, making it an acceptable starting point. As for the User Average and Movie Average, their RMSE and MAE values fall in between the Global Average and Linear Combination, indicating that personalized recommendations based on user or movie averages can be reasonably accurate.\n",
    "Overall, the differences in RMSE and MAE values among the four approaches are relatively small, indicating that these naive approaches provide competitive in accuracy results for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203072b5",
   "metadata": {},
   "source": [
    "# UV Matrix Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba274610",
   "metadata": {},
   "source": [
    "### k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c18919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsme(y_true, y_pred):\n",
    "    return math.sqrt(np.square(np.subtract(y_true, y_pred)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf19e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):\n",
    "    return np.absolute(np.subtract(y_true, y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d34143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, X, y, k = 5):\n",
    "    # Shuffle data\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    \n",
    "    # Split data\n",
    "    n = len(X)\n",
    "    X_folds = [X.iloc[(i - 1) * (n // k):i * (n // k),:] for i in range(1, k + 1)]\n",
    "    y_folds = [y.iloc[(i - 1) * (n // k):i * (n // k)] for i in range(1, k + 1)]\n",
    "    \n",
    "    # Initialize array to store RSME calculations\n",
    "    rmse_ = np.empty((0, 2), float)\n",
    "    mae_ = np.empty((0, 2), float)\n",
    "    \n",
    "    \n",
    "    for i, (X_test, y_test) in enumerate(zip(X_folds, y_folds)):\n",
    "        X_train = pd.concat([X for j, X in enumerate(X_folds) if i != j])\n",
    "        y_train = pd.concat([y for j, y in enumerate(y_folds) if i != j])\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on training and test set\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate errors\n",
    "        rmse_train = rsme(y_train, y_train_pred) \n",
    "        rmse_test = rsme(y_test, y_test_pred)\n",
    "        mae_train = mae(y_train, y_train_pred)\n",
    "        mae_test = mae(y_test, y_test_pred)\n",
    "        \n",
    "        rmse_ = np.append(rmse_, np.array([[rmse_train, rmse_test]]), axis = 0)\n",
    "        mae_ = np.append(mae_, np.array([[mae_train, mae_test]]), axis = 0)\n",
    "    \n",
    "    rmse_train_aver = rmse_[:, 0].mean()\n",
    "    rmse_test_aver = rmse_[:, 1].mean()\n",
    "    mae_train_aver = mae_[:, 0].mean()\n",
    "    mae_test_aver = mae_[:, 1].mean()\n",
    "    \n",
    "    return (rmse_train_aver, rmse_test_aver), (mae_train_aver, mae_test_aver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b8b4e",
   "metadata": {},
   "source": [
    "## Naive Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveApproach:\n",
    "    def __init__(self, fit = lambda *args: None, predict = lambda *args: None):\n",
    "        self.predict = predict\n",
    "        self.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ratings[['UserID', 'MovieID']]\n",
    "y = ratings['Rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d51a69",
   "metadata": {},
   "source": [
    "### Global average rating\n",
    "\n",
    "$$\n",
    "R_{global}(User, Item)=mean(\\text{all ratings})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448edb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.581564453029317"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_global = ratings['Rating'].mean()\n",
    "r_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSME: 1.117\n",
      "MAE: 0.934\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "global_average_rating_model = NaiveApproach(predict=lambda *args: r_global)\n",
    "(_, rmse_test) , (_, mae_test) = cross_validation(global_average_rating_model, X, y)\n",
    "\n",
    "print(f'RSME: {round(rmse_test, 3)}\\nMAE: {round(mae_test, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4fe06",
   "metadata": {},
   "source": [
    "### Average rating per Item\n",
    "\n",
    "$$\n",
    "R_{item}(User, Item)=mean(\\text{all ratings for Item})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2359b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_item(item):\n",
    "    ratings_item = ratings[(ratings['MovieID'] == item)]\n",
    "    return ratings_item['Rating'].mean() if len(ratings_item) > 0 else r_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average rates of all items\n",
    "R_i = {}\n",
    "\n",
    "for movieID in ratings['MovieID'].unique():\n",
    "    R_i[movieID] = r_item(movieID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceee87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSME: 0.975\n",
      "MAE: 0.779\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "average_rating_item_model = NaiveApproach(predict=lambda X: X['MovieID'].apply(lambda x: R_i[x]))\n",
    "(_, rmse_test) , (_, mae_test) = cross_validation(average_rating_item_model, X, y)\n",
    "\n",
    "print(f'RSME: {round(rmse_test, 3)}\\nMAE: {round(mae_test, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b588e65",
   "metadata": {},
   "source": [
    "### Average rating per User\n",
    "\n",
    "$$\n",
    "R_{User}(User, Item)=mean(\\text{all ratings for User})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_user(user):\n",
    "    ratings_user = ratings[(ratings['UserID'] == user)]\n",
    "    return ratings_user['Rating'].mean() if len(ratings_user) > 0 else r_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average rates of all users\n",
    "R_u = {}\n",
    "\n",
    "for userID in ratings['UserID'].unique():\n",
    "    R_u[userID] = r_user(userID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99087551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSME: 1.028\n",
      "MAE: 0.823\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "average_rating_user_model = NaiveApproach(predict=lambda X: X['UserID'].apply(lambda x: R_u[x]))\n",
    "(_, rmse_test) , (_, mae_test) = cross_validation(average_rating_user_model, X, y)\n",
    "\n",
    "print(f'RSME: {round(rmse_test, 3)}\\nMAE: {round(mae_test, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85072ea6",
   "metadata": {},
   "source": [
    "### Optimal Linear Combination of 2 averages\n",
    "\n",
    "$$\n",
    "R_{user-item}(User, Item) = \\alpha * R_{user}(User, Item) + \\beta * R_{item}(User, Item) + \\gamma\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa56f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearCombination:\n",
    "    def fit(self, X, y):\n",
    "        y = y.to_numpy()\n",
    "        \n",
    "        X_u = X['UserID'].apply(lambda x: R_u[x]).to_numpy()\n",
    "        X_i = X['MovieID'].apply(lambda x: R_i[x]).to_numpy()\n",
    "        X = np.vstack([X_u, X_i])\n",
    "        \n",
    "        A = np.vstack([X, np.ones(X.shape[1])]).T\n",
    "        self.alpha, self.beta, self.gamma = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "    \n",
    "    def formula(self, r_u, r_i):\n",
    "        return \n",
    "    \n",
    "    def predict(self, X):\n",
    "        r_u = X['UserID'].apply(lambda x: R_u[x])\n",
    "        r_i = X['MovieID'].apply(lambda x: R_i[x])\n",
    "        \n",
    "        return self.alpha * r_u + self.beta * r_i + self.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d5b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSME: 0.916\n",
      "MAE: 0.726\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "linear_combination_model = LinearCombination()\n",
    "(_, rmse_test) , (_, mae_test) = cross_validation(linear_combination_model, X, y)\n",
    "\n",
    "print(f'RSME: {round(rmse_test, 3)}\\nMAE: {round(mae_test, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d80867",
   "metadata": {},
   "source": [
    "## UV Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class roundrobin:\n",
    "    def __init__(self, U, V):\n",
    "        self.U = U\n",
    "        self.V = V\n",
    "        \n",
    "        self.maxrows1, self.maxcols1 = U.shape[0], U.shape[1]\n",
    "        self.maxrows2, self.maxcols2 = V.shape[0], V.shape[1]\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.iterU = True\n",
    "        self.row1 = self.col1 = 0\n",
    "        self.row2 = self.col2 = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.iterU:\n",
    "            if self.row1 > self.maxrows1 - 1:\n",
    "                self.row1 = 0\n",
    "                self.col1 += 1\n",
    "                if self.col1 > self.maxcols1 - 1:\n",
    "                    raise StopIteration\n",
    "\n",
    "            next_ = (self.iterU, self.row1, self.col1)\n",
    "            self.iterU = False\n",
    "            self.row1 += 1\n",
    "            return next_\n",
    "        else:    \n",
    "            if self.row2 > self.maxrows2 - 1:\n",
    "                self.row2 = 0\n",
    "                self.col2 += 1\n",
    "                if self.col2 > self.maxcols2 - 1:\n",
    "                    raise StopIteration\n",
    "\n",
    "            next_ = (self.iterU, self.row2, self.col2)\n",
    "            self.iterU = True\n",
    "            self.row2 += 1\n",
    "            return next_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299a386",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n Users\n",
    "unique_users = ratings['UserID'].unique()\n",
    "n = len(unique_users)\n",
    "\n",
    "# m Items\n",
    "unique_items = ratings['MovieID'].unique()\n",
    "m = len(unique_items)\n",
    "\n",
    "# Initialize utility matrix\n",
    "M = np.full((n,m), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map ids to index & vice-versa\n",
    "user_to_index = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "index_to_user = {idx: user_id for idx, user_id in enumerate(unique_users)}\n",
    "\n",
    "item_to_index = {item_id: idx for idx, item_id in enumerate(unique_items)}\n",
    "index_to_item = {idx: item_id for idx, item_id in enumerate(unique_items)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8c0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate matrix M\n",
    "for _, row in ratings.iterrows():\n",
    "    user_id = row['UserID']\n",
    "    item_id = row['MovieID']\n",
    "    \n",
    "    norm = (R_u[user_id] + R_i[item_id]) / 2\n",
    "    \n",
    "    M[user_to_index[user_id], item_to_index[item_id]] = row['Rating'] - norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752de596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def rmse_2d(M, M_pred):\n",
    "    mask = ~np.isnan(M)\n",
    "    return math.sqrt(np.mean(np.square(M - M_pred), where=~np.isnan(M)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4091633",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of short sizes of U & V\n",
    "d = 10\n",
    "\n",
    "# Mean of non-blank values in M\n",
    "a = np.mean(M, where=~np.isnan(M))\n",
    "\n",
    "initial_value = math.sqrt(a/d)\n",
    "\n",
    "U = np.full((n, d), initial_value)\n",
    "V = np.full((d, m), initial_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222130b0",
   "metadata": {},
   "source": [
    "### Performing the Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35845f20",
   "metadata": {},
   "source": [
    "#### Optimizing elements of U\n",
    "$$\n",
    "x = \\frac{\\sum_{j} v_{sj}(m_{rj} - \\sum_{k \\neq s} u_{rk}v_{kj})}{\\sum_{j} v^{2}_{sj}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ba658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_x(M, U, V, r, s):\n",
    "    urk = np.delete(U[r, :], s, axis=0)\n",
    "    vkj = np.delete(V, s, axis=0)\n",
    "\n",
    "    vsj = V[s, :]\n",
    "\n",
    "    numerator = vsj * (M[r, :] - np.dot(urk.reshape(-1,1).T, vkj))\n",
    "    numerator = np.sum(numerator, where=~np.isnan(numerator))\n",
    "\n",
    "    denominator = np.square(vsj)\n",
    "    denominator = np.sum(denominator)\n",
    "\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b1a80",
   "metadata": {},
   "source": [
    "#### Optimizing elements of V\n",
    "$$\n",
    "y = \\frac{\\sum_{i} u_{ir}(m_{is} - \\sum_{k \\neq r} u_{ik}v_{ks})}{\\sum_{i} u^{2}_{ir}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14857424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_y(M, U, V, r, s):   \n",
    "    uik = np.delete(U, r, axis=1)\n",
    "    vks = np.delete(V[:,s], r, axis=0)\n",
    "    \n",
    "    uir = U[:,r]\n",
    "   \n",
    "    numerator = uir * (M[:,s] - np.dot(uik, vks))\n",
    "    numerator = np.sum(numerator, where=~np.isnan(numerator))\n",
    "\n",
    "    denominator = np.square(uir)\n",
    "    denominator = np.sum(denominator, where=~np.isnan(denominator))\n",
    "\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206bb1c9",
   "metadata": {},
   "source": [
    "#### Optimizing decompositions U and V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84911ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial RMSE: 0.9515933193173758\n"
     ]
    }
   ],
   "source": [
    "UV = np.dot(U, V)\n",
    "print(f'Initial RMSE: {rmse_2d(M, UV)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d1318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▉                                         | 1/15 [00:14<03:16, 14.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▊                                      | 2/15 [00:26<02:53, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▊                                   | 3/15 [00:40<02:39, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████▋                                | 4/15 [00:52<02:22, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████▋                             | 5/15 [01:04<02:05, 12.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 6/15 [01:18<01:56, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████▌                       | 7/15 [01:33<01:49, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████▍                    | 8/15 [01:47<01:35, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████▍                 | 9/15 [02:00<01:21, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████▋              | 10/15 [02:12<01:05, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████████████████████████████▌           | 11/15 [02:25<00:52, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████▍        | 12/15 [02:39<00:39, 13.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|█████████████████████████████████████▎     | 13/15 [02:51<00:26, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "same_err = 0\n",
    "last_err = float('inf')\n",
    "for _ in tqdm(range(15)):\n",
    "    # Stop if error doesn't improve for 3 epochs\n",
    "    if same_err >= 3:\n",
    "      break\n",
    "\n",
    "    # Training\n",
    "    for isU, r, s in roundrobin(U, V):\n",
    "        if isU:\n",
    "            U[r,s] = optimize_x(M, U, V, r, s)\n",
    "        else:\n",
    "            V[r,s] = optimize_y(M, U, V, r, s)\n",
    "    \n",
    "    # Monitoring\n",
    "    UV = np.dot(U, V)\n",
    "    err = round(rmse_2d(M, UV),3)\n",
    "    if err == last_err:\n",
    "      same_err += 1\n",
    "    last_err = err\n",
    "\n",
    "    print(f'RMSE: {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1bbf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RMSE: 0.8910636397215524\n"
     ]
    }
   ],
   "source": [
    "UV = np.dot(U, V)\n",
    "print(f'Optimized RMSE: {rmse_2d(M, UV)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4996dbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSME: 0.891\n",
      "MAE: 0.708\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "def decomp_predict(X_test):\n",
    "    y_pred = []\n",
    "    for _, x in X_test.iterrows():\n",
    "        user_id = x['UserID']\n",
    "        item_id = x['MovieID']\n",
    "        norm = (R_u[user_id] + R_i[item_id]) / 2\n",
    "        y_pred.append(UV[user_to_index[user_id], item_to_index[item_id]] + norm)\n",
    "        \n",
    "    return y_pred\n",
    "    \n",
    "uv_decomposition_model = NaiveApproach(predict=decomp_predict)\n",
    "(_, rmse_test) , (_, mae_test) = cross_validation(uv_decomposition_model, X, y)\n",
    "\n",
    "print(f'RSME: {round(rmse_test, 3)}\\nMAE: {round(mae_test, 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6285fe",
   "metadata": {},
   "source": [
    "# Matrix Factorization with Gradient Descent and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f8201",
   "metadata": {},
   "source": [
    "For this part of the report, matrix factorization algorithm is going to be implemented in a similar way with the one described in the gravity-Tikk.pdf paper. To be more specific, this algorithm takes as an input a Ratings Matrix, where each row i represents users, each column j represents movies and the (i,j) element of the matrix is the rating of user i to the movie j. However, this matrix has many elements that are zeros, since not all users have rated all the movies.  So, the algorithm aims to predict the zero elements of the matrix by filling the whole Ratings matrix. To achieve that, it decompose the Ratings matrix into the product of two matrices U (for users) and M (for movies). The U matrix has as many rows as the users and K columns, whereas the M matrix has as many columns as the movies are and K rows. The value of K is going to be chosen, based on the smaller achieved test root mean squares error(RMSE) and mean absolute error(MAE), after some experiments are conducted. The goal of the algorithm is to minimize the difference between the predicted ratings matrix and the real one ratings matrix, taking into account, while calculating that difference, only the (i,j) elements of the matrices that were not initialy zero in the real ratings matrix. The problem is solved using a gradient method to adjust elements of U and M in each iteration, so that to minimize the error between the real rating and the predicted rating. Additionally, regularization is applied to prevent overfitting. The first thing that the algorithm does is to initialize the  U and M matrices randomly and then choose learning rate (η), regularization factor (λ), K and max number of iterations. Then, it iteratively updates U and M untill the RMSE on the probe subset(list of [i,j,rating] elements, where the rating is not 0 on the real ratings matrix) stop decreasing for two consecutive iterations or untill the max number of iterations chosen. In addition to the gravity-Tikk.pdf paper, the gradients here are penalized to not exceed the value of 1 and the predicted ratings matrix is squeezed to [1,5], meaning that values less than 1 are changed to be 1 and values greater than 5, are changed to be 5. Also, if the RMSE does not reduce for two consecutive iterations but it is not lower than 1, the algorithm continues untill it reaches the maximum number of iterations.\n",
    "\n",
    "Since this algorithm takes hours to run, the experiments are run using the parameters that are reported on the MyMedialite website: num_factors=10, num_iter=75, regilarization=0/05, learnr_rate=0.05.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a68bc",
   "metadata": {},
   "source": [
    "Before implementing the algorthim though, some preprocessing of the data should be done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c563e",
   "metadata": {},
   "source": [
    "## Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8e86c",
   "metadata": {},
   "source": [
    "The users and movies dataframes are encoded in order to have continues userID and movieID values without any gaps. Their indexes are also reset to be continuous. That is because we want each row of the Ratings matrix to be able to be mapped with the respective userID and each column of the Ratings matrix to be able to be mapped with the respective row of the movieID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120547f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "unique_users_ids = ratings.loc[:,'UserID'].unique().copy()\n",
    "# Assuming your movies features matrix is named 'movies_features_df'\n",
    "filtered_users = users[users.loc[:,'UserID'].isin(unique_users_ids)].copy()\n",
    "\n",
    "unique_movies_ids = ratings.loc[:,'MovieID'].unique().copy()\n",
    "# Assuming your movies features matrix is named 'movies_features_df'\n",
    "filtered_movies = movies[movies.loc[:,'MovieID'].isin(unique_movies_ids)].copy()\n",
    "\n",
    "\n",
    "filtered_movies.loc[:, 'MovieID'] = le.fit_transform(filtered_movies['MovieID']).copy()\n",
    "filtered_users.loc[:, 'UserID'] = le.fit_transform(filtered_users['UserID']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_movies.reset_index(drop=True, inplace=True)\n",
    "filtered_users.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f733636",
   "metadata": {},
   "source": [
    "Finally the Ratings matrix is created as a dataframe and named as 'user_movie_retings'. Each row represents a UserID and each column a MovieID. In this matrix all the users ratings are presented. The zeros in the matrix represent movies that the specific user have not rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_movie_ratings = ratings.pivot(index='UserID', columns='MovieID', values='Rating').fillna(0)\n",
    "\n",
    "# Encode columns (MovieIDs)\n",
    "user_movie_ratings.columns = le.fit_transform(user_movie_ratings.columns)\n",
    "\n",
    "# Encode row names (UserIDs)\n",
    "user_movie_ratings.index = le.fit_transform(user_movie_ratings.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de2d23b",
   "metadata": {},
   "source": [
    "## Implementing the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ba15b",
   "metadata": {},
   "source": [
    "\n",
    "Here a class named MatrixFactorization is created to implement the aformentioned algorithm. It is initialized with the values of K, max iterations, η and λ. Then, the algorthm is performed in the .fit instance, using operations on matrices in order to avoid nested loops which are slow. U and M are initialized using the random library. The class has also instances to return the predicted ratings matrix, the mae and rmse of the train set and the rmse and mae on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "    def __init__(self, K=10, max_iter=75, eta=0.005, lamda=0.05):\n",
    "        self.K = K  # Features of matrices\n",
    "        self.max_iter = max_iter  # Maximum number of iterations\n",
    "        self.eta = eta  # Learning rate\n",
    "        self.lamda = lamda  # Regularization parameter\n",
    "\n",
    "    def fit(self, train_set):\n",
    "        self.zero_rows=np.where(~train_set.values.any(axis=1))[0] #\n",
    "        self.zero_columns=np.where(~train_set.values.any(axis=0))[0]\n",
    "\n",
    "        prev_rmse = float('inf')\n",
    "        consecutive_increase_count = 0\n",
    "\n",
    "        probe_subset = [(i, j, train_set.iloc[i, j]) for i in range(len(train_set)) for j in range(len(train_set.columns)) if train_set.iloc[i, j] > 0]\n",
    "\n",
    "        N, Z = train_set.shape  # Dimensions of the user-item matrix\n",
    "        self.train_set_values=train_set.values\n",
    "\n",
    "        self.U = np.random.rand(N, self.K)  # Initialize user matrix randomly\n",
    "        self.M = np.random.rand(Z, self.K)  # Initialize movie matrix randomly\n",
    "\n",
    "        raw_predictions = np.dot(self.U, self.M.T)\n",
    "        self.predictions =  np.clip(raw_predictions, 1, 5)\n",
    "\n",
    "        plot_rmse_train=[]\n",
    "        plot_rmse_test=[]\n",
    "        for step in range(self.max_iter):\n",
    "            # Calculate errors for non-zero entries in train_set\n",
    "            train_values = np.where(np.isnan(train_set.values), 0, train_set.values) #put zero where there is nan\n",
    "            U_values = np.where(np.isnan(self.U), 0, self.U)\n",
    "            M_T_values = np.where(np.isnan(self.M.T), 0, self.M.T)\n",
    "\n",
    "            errors = (train_values > 0) * (train_values - np.dot(U_values, M_T_values)) #erors in all positions of array without zeros in train\n",
    "\n",
    "\n",
    "            # Calculate gradients for U and M using matrix operations\n",
    "            gradient_U = 2 * (np.dot(errors, self.M) - self.lamda * self.U)\n",
    "            gradient_M = 2 * (np.dot(errors.T, self.U) - self.lamda * self.M)\n",
    "\n",
    "\n",
    "            max_gradient = 1.0  # Set an appropriate maximum gradient value\n",
    "            gradient_U = np.clip(gradient_U, -max_gradient, max_gradient)\n",
    "            gradient_M = np.clip(gradient_M, -max_gradient, max_gradient)\n",
    "\n",
    "            self.U += self.eta * gradient_U\n",
    "            self.M += self.eta * gradient_M\n",
    "\n",
    "            raw_predictions = np.dot(self.U, self.M.T)\n",
    "            self.predictions =  np.clip(raw_predictions, 1, 5)\n",
    "\n",
    "            # Calculate RMSE on the probe subset\n",
    "            train_probe_rmse = self.get_rmse(probe_subset)\n",
    "            train_probe_mae = self.get_mae(probe_subset)\n",
    "\n",
    "\n",
    "            print(\"Iteration:\", step + 1, \" train RMSE:\", train_probe_rmse, \"train MAE:\", train_probe_mae)\n",
    "\n",
    "            plot_rmse_train.append(train_probe_rmse)\n",
    "\n",
    "            # Check for convergence by comparing RMSE with the previous iteration\n",
    "            if train_probe_rmse >= prev_rmse:\n",
    "                consecutive_increase_count += 1\n",
    "                if consecutive_increase_count >= 2 and train_probe_rmse<=1:\n",
    "                    print(\"Converged. RMSE did not decrease for 2 consecutive iterations.\")\n",
    "                    print('Train RMSE:',train_probe_rmse)\n",
    "                    print('Train MAE:',train_probe_mae)\n",
    "                    break\n",
    "            else:\n",
    "                consecutive_increase_count = 0\n",
    "\n",
    "            prev_rmse = train_probe_rmse\n",
    "\n",
    "\n",
    "        raw_predictions = np.dot(self.U, self.M.T)\n",
    "        self.predictions =  np.clip(raw_predictions, 1, 5)\n",
    "\n",
    "\n",
    "        return train_probe_rmse,train_probe_mae, plot_rmse_train\n",
    "\n",
    "    def predict(self):\n",
    "        return self.predictions\n",
    "\n",
    "\n",
    "    def test_rmse_mae(self, test_data):\n",
    "        test_data = test_data.values\n",
    "        nR = self.predictions\n",
    "\n",
    "        non_zero_mask = (test_data > 0) & np.logical_not(np.isin(np.arange(len(test_data)), self.zero_rows)[:, np.newaxis]) & np.logical_not(np.isin(np.arange(len(test_data[0])), self.zero_columns))\n",
    "        actual_ratings = test_data[non_zero_mask]\n",
    "        predicted_ratings = nR[non_zero_mask]\n",
    "\n",
    "        rmse = np.sqrt(np.mean((predicted_ratings - actual_ratings) ** 2))\n",
    "        mae = np.mean(np.abs(predicted_ratings - actual_ratings))\n",
    "\n",
    "        return rmse, mae\n",
    "\n",
    "\n",
    "    def get_rmse(self, probe_subset):\n",
    "        rmse = 0\n",
    "        count = 0\n",
    "        for i, j, actual_rating in probe_subset:\n",
    "            predicted_rating = self.predictions [i,j]\n",
    "            rmse += (predicted_rating - actual_rating) ** 2\n",
    "            count += 1\n",
    "        rmse = np.sqrt(rmse / count)\n",
    "        return rmse\n",
    "\n",
    "    def get_mae(self, probe_subset):\n",
    "        mae = 0\n",
    "        count = 0\n",
    "        for i, j, actual_rating in probe_subset:\n",
    "            predicted_rating = self.predictions [i,j]\n",
    "            mae += np.abs(predicted_rating - actual_rating)\n",
    "            count += 1\n",
    "        mae = mae / count\n",
    "        return mae\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6160e072",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afbeb11",
   "metadata": {},
   "source": [
    "\n",
    "For the results to be more accurate 5-fold cross validation is performed using a function named cross_validation. This function takes as input the model, which is from the MatrixFactorization class, the dataphrame with the real ratings and the number of folds that the dataphrame is going to be split into. To split the data into 4 train sets and 1 test set, a list of tubles is created with (user,movie, rating) elements, for the elements of the matrix that are non zero. Then the train matrix, which has the same shape as the original matrix is filled with the 4/5 of those data and the test matrix, which also has the same shape as the original matrix is filled with the 1/5 of those data. This happens for 5 times in order to have different combinations of train and test sets. If a user or a movie is not presented in the train matrix (i.e all elements of a specific row (user) are filled with zeros or all elements of a specific column (movie) are filled with zeros), then that user and that movie are not taking into account when calculation the RMSE and MAE on the test data. In the cross validation function, the fit instance of the model is called for every one of the five times that the spliting is performed and for each time the test RMSE and MAE are calculated and printed. At the end, the avairage of those values is also printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, data_df, num_folds=5):\n",
    "\n",
    "\n",
    "    num_users, num_movies = data_df.shape\n",
    "\n",
    "    # Create a list of (user, movie, rating) tuples from the DataFrame\n",
    "    data = [(user, movie, data_df.iloc[user, movie]) for user in range(num_users) for movie in range(num_movies) if data_df.iloc[user, movie] != 0]\n",
    "\n",
    "\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize empty DataFrames for train and test sets\n",
    "    train_sets = [pd.DataFrame(np.zeros((num_users, num_movies))) for _ in range(num_folds)]\n",
    "    test_sets = [pd.DataFrame(np.zeros((num_users, num_movies))) for _ in range(num_folds)]\n",
    "\n",
    "    # Iterate through the folds\n",
    "    train_sum_rmse = 0\n",
    "    train_sum_mae = 0\n",
    "\n",
    "    test_sum_rmse = 0\n",
    "    test_sum_mae = 0\n",
    "\n",
    "    plot_train=[]\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(data), 1):\n",
    "        train_data = [data[i] for i in train_index]\n",
    "        test_data = [data[i] for i in test_index]\n",
    "\n",
    "        # Fill train and test DataFrames with ratings from the respective data tuples\n",
    "        for user, movie, rating in train_data:\n",
    "            train_sets[fold - 1].iloc[user, movie] = rating\n",
    "\n",
    "        for user, movie, rating in test_data:\n",
    "            test_sets[fold - 1].iloc[user, movie] = rating\n",
    "\n",
    "        mf = model\n",
    "        (train_rmse,train_mae, plot_train_rmse) = mf.fit(train_sets[fold - 1])\n",
    "\n",
    "        plot_train.append(plot_train_rmse)\n",
    "\n",
    "        (test_rmse,test_mae) =mf.test_rmse_mae(test_sets[fold - 1])\n",
    "\n",
    "        print('Final train RMSE for fold:',fold,':', train_rmse)\n",
    "        print('Final train MAE for fold:',fold,':', train_mae)\n",
    "\n",
    "        print('Final test RMSE for fold:',fold,':', test_rmse)\n",
    "        print('Final test MAE for fold:',fold,':', test_mae)\n",
    "\n",
    "        train_sum_rmse = train_sum_rmse + train_rmse\n",
    "        train_sum_mae = train_sum_mae + train_mae\n",
    "\n",
    "\n",
    "        test_sum_rmse = test_sum_rmse + test_rmse\n",
    "        test_sum_mae = test_sum_mae + test_mae\n",
    "\n",
    "    overall_train_rmse=train_sum_rmse/num_folds\n",
    "    overall_train_mae=train_sum_mae/num_folds\n",
    "\n",
    "    overall_test_rmse=test_sum_rmse/num_folds\n",
    "    overall_test_mae=test_sum_mae/num_folds\n",
    "\n",
    "    print('Overall Train RMSE:',overall_train_rmse)\n",
    "    print('Overall Train MAE:',overall_train_mae)\n",
    "\n",
    "    print('Overall Test RMSE:',overall_test_rmse)\n",
    "    print('Overall Test MAE:',overall_test_mae)\n",
    "\n",
    "    return plot_train,  overall_test_rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dfb0c7",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c3bd4",
   "metadata": {},
   "source": [
    "Time to run the experiments. The cross validation function is called 3 times, each time with a different value of K in order to compare their performance. The values that are tried are K=10,15,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=user_movie_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b975cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  train RMSE: 1.6783192258639683 train MAE: 1.4147181773926547\n",
      "Iteration: 2  train RMSE: 1.6479166447322926 train MAE: 1.3857431377176015\n",
      "Iteration: 3  train RMSE: 1.6178707885973316 train MAE: 1.357268823823823\n",
      "Iteration: 4  train RMSE: 1.5882224957119933 train MAE: 1.3293257874134052\n",
      "Iteration: 5  train RMSE: 1.5590149530691895 train MAE: 1.3019322849275268\n",
      "Iteration: 6  train RMSE: 1.5302905248228964 train MAE: 1.2751277983989482\n",
      "Iteration: 7  train RMSE: 1.5020821218525977 train MAE: 1.2489478754249856\n",
      "Iteration: 8  train RMSE: 1.4744220095218623 train MAE: 1.2234138218132764\n",
      "Iteration: 9  train RMSE: 1.4473403276580874 train MAE: 1.1985290946243747\n",
      "Iteration: 10  train RMSE: 1.42086576380658 train MAE: 1.1743041328817803\n",
      "Iteration: 11  train RMSE: 1.3950289983131319 train MAE: 1.1507640889986444\n",
      "Iteration: 12  train RMSE: 1.3698550120932824 train MAE: 1.1279264018827824\n",
      "Iteration: 13  train RMSE: 1.3453694190477081 train MAE: 1.1058124670734306\n",
      "Iteration: 14  train RMSE: 1.3215944688080912 train MAE: 1.084414304677851\n",
      "Iteration: 15  train RMSE: 1.2985525123978419 train MAE: 1.0637476952380964\n",
      "Iteration: 16  train RMSE: 1.2762624483386102 train MAE: 1.0438017924667582\n",
      "Iteration: 17  train RMSE: 1.254729503892181 train MAE: 1.02459303609834\n",
      "Iteration: 18  train RMSE: 1.233964251297079 train MAE: 1.006124502382504\n",
      "Iteration: 19  train RMSE: 1.2139750024434592 train MAE: 0.9884088235026183\n",
      "Iteration: 20  train RMSE: 1.194763522431225 train MAE: 0.971432177755859\n",
      "Iteration: 21  train RMSE: 1.1763352459613097 train MAE: 0.955168391831567\n",
      "Iteration: 22  train RMSE: 1.1586942393212305 train MAE: 0.9396494698358371\n",
      "Iteration: 23  train RMSE: 1.1418344424013753 train MAE: 0.9248412610300704\n",
      "Iteration: 24  train RMSE: 1.1257517487062874 train MAE: 0.9107373409825815\n",
      "Iteration: 25  train RMSE: 1.1104350650000767 train MAE: 0.8973598438278838\n",
      "Iteration: 26  train RMSE: 1.0958726631595812 train MAE: 0.8846198384183657\n",
      "Iteration: 27  train RMSE: 1.082051911239559 train MAE: 0.8725786977058966\n",
      "Iteration: 28  train RMSE: 1.0689481514929453 train MAE: 0.8611225197749088\n",
      "Iteration: 29  train RMSE: 1.0565500181253753 train MAE: 0.8503494337160107\n",
      "Iteration: 30  train RMSE: 1.0448300614247872 train MAE: 0.8400993537805631\n",
      "Iteration: 31  train RMSE: 1.033777360246767 train MAE: 0.8305468817218776\n",
      "Iteration: 32  train RMSE: 1.0233551561625212 train MAE: 0.8213870907579934\n",
      "Iteration: 33  train RMSE: 1.0135502317545209 train MAE: 0.8130111480584779\n",
      "Iteration: 34  train RMSE: 1.0043130995966385 train MAE: 0.8048358250577606\n",
      "Iteration: 35  train RMSE: 0.9956610558756732 train MAE: 0.7976147090453726\n",
      "Iteration: 36  train RMSE: 0.9875045181045137 train MAE: 0.790184963245737\n",
      "Iteration: 37  train RMSE: 0.979907802486083 train MAE: 0.7840221597234843\n",
      "Iteration: 38  train RMSE: 0.9727464417855818 train MAE: 0.7772685854102872\n",
      "Iteration: 39  train RMSE: 0.9661070982474331 train MAE: 0.7720757395289843\n",
      "Iteration: 40  train RMSE: 0.9598286996220938 train MAE: 0.7658854297076091\n",
      "Iteration: 41  train RMSE: 0.9540345846604691 train MAE: 0.7616567473423922\n",
      "Iteration: 42  train RMSE: 0.9485355932123607 train MAE: 0.7559181723885537\n",
      "Iteration: 43  train RMSE: 0.9434976789198629 train MAE: 0.7525336065782248\n",
      "Iteration: 44  train RMSE: 0.9387008469028255 train MAE: 0.7471880035898465\n",
      "Iteration: 45  train RMSE: 0.9343517870958726 train MAE: 0.7446278774388391\n",
      "Iteration: 46  train RMSE: 0.9301855021515625 train MAE: 0.7395362716049048\n",
      "Iteration: 47  train RMSE: 0.9264448166228595 train MAE: 0.7377626754221567\n",
      "Iteration: 48  train RMSE: 0.922834169383218 train MAE: 0.7328862253378674\n",
      "Iteration: 49  train RMSE: 0.9196314757637931 train MAE: 0.731762463116522\n",
      "Iteration: 50  train RMSE: 0.9165225011082793 train MAE: 0.7270865675259158\n",
      "Iteration: 51  train RMSE: 0.9137998759229985 train MAE: 0.7266005482135512\n",
      "Iteration: 52  train RMSE: 0.9111264920490015 train MAE: 0.7219982346871558\n",
      "Iteration: 53  train RMSE: 0.9088399441461807 train MAE: 0.7221725452618029\n",
      "Iteration: 54  train RMSE: 0.9065406354660243 train MAE: 0.7176415631400026\n",
      "Iteration: 55  train RMSE: 0.9046265654884413 train MAE: 0.7183711455716799\n",
      "Iteration: 56  train RMSE: 0.902646896298139 train MAE: 0.7139441760225937\n",
      "Iteration: 57  train RMSE: 0.9010547825646467 train MAE: 0.7151800095269715\n",
      "Iteration: 58  train RMSE: 0.8993479419867856 train MAE: 0.7107978496459753\n",
      "Iteration: 59  train RMSE: 0.8980150007913988 train MAE: 0.7124172527183469\n",
      "Iteration: 60  train RMSE: 0.8965469101993513 train MAE: 0.7081289109079811\n",
      "Iteration: 61  train RMSE: 0.8954400107975661 train MAE: 0.7100956417128882\n",
      "Iteration: 62  train RMSE: 0.8941646021535637 train MAE: 0.7058590695519602\n",
      "Iteration: 63  train RMSE: 0.8932390662725899 train MAE: 0.7081167283538329\n",
      "Iteration: 64  train RMSE: 0.8921071022344766 train MAE: 0.7039364985624391\n",
      "Iteration: 65  train RMSE: 0.8913317772877414 train MAE: 0.7064446727331379\n",
      "Iteration: 66  train RMSE: 0.8903070290387051 train MAE: 0.7022852909034969\n",
      "Iteration: 67  train RMSE: 0.8896537601264297 train MAE: 0.7050211536363038\n",
      "Iteration: 68  train RMSE: 0.8887022035690785 train MAE: 0.7008646965144052\n",
      "Iteration: 69  train RMSE: 0.8881364252469378 train MAE: 0.7037463887973383\n",
      "Iteration: 70  train RMSE: 0.8872394725902943 train MAE: 0.699580668669002\n",
      "Iteration: 71  train RMSE: 0.8867243503187723 train MAE: 0.7025328042035136\n",
      "Iteration: 72  train RMSE: 0.8858643817671068 train MAE: 0.698400121378842\n",
      "Iteration: 73  train RMSE: 0.8853871334565838 train MAE: 0.7013986721117954\n",
      "Iteration: 74  train RMSE: 0.8845572745599785 train MAE: 0.6972861890312265\n",
      "Iteration: 75  train RMSE: 0.88411271697189 train MAE: 0.700321643587556\n",
      "Final train RMSE for fold: 1 : 0.88411271697189\n",
      "Final train MAE for fold: 1 : 0.700321643587556\n",
      "Final test RMSE for fold: 1 : 0.9238766844158867\n",
      "Final test MAE for fold: 1 : 0.7312291035320115\n",
      "Iteration: 1  train RMSE: 1.6510920475311748 train MAE: 1.3862973779187084\n",
      "Iteration: 2  train RMSE: 1.6211324434391692 train MAE: 1.3578858817573676\n",
      "Iteration: 3  train RMSE: 1.5915790498370583 train MAE: 1.3300021074042054\n",
      "Iteration: 4  train RMSE: 1.5624663584078415 train MAE: 1.3027068153361154\n",
      "Iteration: 5  train RMSE: 1.533837866062078 train MAE: 1.2760347887501013\n",
      "Iteration: 6  train RMSE: 1.5057301010781512 train MAE: 1.249980854747107\n",
      "Iteration: 7  train RMSE: 1.4781791702517844 train MAE: 1.2245681588027824\n",
      "Iteration: 8  train RMSE: 1.4512149757088437 train MAE: 1.1998219742466467\n",
      "Iteration: 9  train RMSE: 1.4248697181640606 train MAE: 1.1757545794875852\n",
      "Iteration: 10  train RMSE: 1.3991727525531665 train MAE: 1.1524036014963317\n",
      "Iteration: 11  train RMSE: 1.3741488008470313 train MAE: 1.1297926153766744\n",
      "Iteration: 12  train RMSE: 1.349812082410462 train MAE: 1.1079053815066302\n",
      "Iteration: 13  train RMSE: 1.3261787791323976 train MAE: 1.0867294997195007\n",
      "Iteration: 14  train RMSE: 1.3032679202259843 train MAE: 1.0662915725869448\n",
      "Iteration: 15  train RMSE: 1.28109761640167 train MAE: 1.0466010517970323\n",
      "Iteration: 16  train RMSE: 1.2596666281458127 train MAE: 1.0276476512541053\n",
      "Iteration: 17  train RMSE: 1.238982377951894 train MAE: 1.0094259523270077\n",
      "Iteration: 18  train RMSE: 1.2190596798699767 train MAE: 0.9919429591551001\n",
      "Iteration: 19  train RMSE: 1.1999013775411462 train MAE: 0.9751600302836059\n",
      "Iteration: 20  train RMSE: 1.181502819358579 train MAE: 0.9590689137036356\n",
      "Iteration: 21  train RMSE: 1.1638598686550878 train MAE: 0.9436680269337318\n",
      "Iteration: 22  train RMSE: 1.14696882381084 train MAE: 0.9289421813052714\n",
      "Iteration: 23  train RMSE: 1.1308312018162858 train MAE: 0.9148990579261237\n",
      "Iteration: 24  train RMSE: 1.1154278511523295 train MAE: 0.901511727047868\n",
      "Iteration: 25  train RMSE: 1.100759042210611 train MAE: 0.8888106246450341\n",
      "Iteration: 26  train RMSE: 1.0868031257376884 train MAE: 0.8767305365526974\n",
      "Iteration: 27  train RMSE: 1.0735426515240043 train MAE: 0.8652632637289446\n",
      "Iteration: 28  train RMSE: 1.060954601231045 train MAE: 0.8543806418146207\n",
      "Iteration: 29  train RMSE: 1.0490286655956116 train MAE: 0.8440805035338039\n",
      "Iteration: 30  train RMSE: 1.037738387791427 train MAE: 0.8343015862698344\n",
      "Iteration: 31  train RMSE: 1.0270732993890153 train MAE: 0.8250582053514126\n",
      "Iteration: 32  train RMSE: 1.017014299704165 train MAE: 0.8163463542086841\n",
      "Iteration: 33  train RMSE: 1.0075375009826204 train MAE: 0.8080502293296645\n",
      "Iteration: 34  train RMSE: 0.9986294278194072 train MAE: 0.8003428975315267\n",
      "Iteration: 35  train RMSE: 0.9902536751590243 train MAE: 0.7929351686625061\n",
      "Iteration: 36  train RMSE: 0.9824081308316182 train MAE: 0.7862570635086343\n",
      "Iteration: 37  train RMSE: 0.9750368685066382 train MAE: 0.7795367256294371\n",
      "Iteration: 38  train RMSE: 0.9681762949085672 train MAE: 0.7738852591842815\n",
      "Iteration: 39  train RMSE: 0.9617327208921019 train MAE: 0.7677118041515001\n",
      "Iteration: 40  train RMSE: 0.9557832934388458 train MAE: 0.7631285344277359\n",
      "Iteration: 41  train RMSE: 0.9501726505071387 train MAE: 0.7573577889962156\n",
      "Iteration: 42  train RMSE: 0.945038425910485 train MAE: 0.7537891499293177\n",
      "Iteration: 43  train RMSE: 0.9401680778240062 train MAE: 0.7483839643455001\n",
      "Iteration: 44  train RMSE: 0.9357546708130087 train MAE: 0.7457102883176268\n",
      "Iteration: 45  train RMSE: 0.9315479363129996 train MAE: 0.7406250207570813\n",
      "Iteration: 46  train RMSE: 0.9277655454145043 train MAE: 0.7387394881552288\n",
      "Iteration: 47  train RMSE: 0.924136562261767 train MAE: 0.7338863890881502\n",
      "Iteration: 48  train RMSE: 0.9209136506442492 train MAE: 0.7327333113898297\n",
      "Iteration: 49  train RMSE: 0.9177951421378935 train MAE: 0.7280558271690845\n",
      "Iteration: 50  train RMSE: 0.9150757152702943 train MAE: 0.727602645007149\n",
      "Iteration: 51  train RMSE: 0.9123979977597236 train MAE: 0.7230717056754568\n",
      "Iteration: 52  train RMSE: 0.9100947434472942 train MAE: 0.7231961568260259\n",
      "Iteration: 53  train RMSE: 0.9077825748751919 train MAE: 0.718775899392392\n",
      "Iteration: 54  train RMSE: 0.9058385822206902 train MAE: 0.7194229652191195\n",
      "Iteration: 55  train RMSE: 0.9038407001873963 train MAE: 0.7150780303664306\n",
      "Iteration: 56  train RMSE: 0.9021949953701267 train MAE: 0.7161788945742925\n",
      "Iteration: 57  train RMSE: 0.9004532120310913 train MAE: 0.7118872551553694\n",
      "Iteration: 58  train RMSE: 0.8990622009955936 train MAE: 0.7133863748940661\n",
      "Iteration: 59  train RMSE: 0.8975420438776467 train MAE: 0.7091710590635567\n",
      "Iteration: 60  train RMSE: 0.8963515188245035 train MAE: 0.7110126211047585\n",
      "Iteration: 61  train RMSE: 0.894997747722005 train MAE: 0.7068161454430227\n",
      "Iteration: 62  train RMSE: 0.8939658992351505 train MAE: 0.7089021521251041\n",
      "Iteration: 63  train RMSE: 0.8927597483705755 train MAE: 0.7047391310700964\n",
      "Iteration: 64  train RMSE: 0.8918574204580938 train MAE: 0.7070194283448126\n",
      "Iteration: 65  train RMSE: 0.8907589412157199 train MAE: 0.7028973037405096\n",
      "Iteration: 66  train RMSE: 0.8899714139695682 train MAE: 0.7053383010141101\n",
      "Iteration: 67  train RMSE: 0.8889647729427925 train MAE: 0.7012562385141572\n",
      "Iteration: 68  train RMSE: 0.8882772183970347 train MAE: 0.7038544880729061\n",
      "Iteration: 69  train RMSE: 0.8873358755998055 train MAE: 0.6997841474051045\n",
      "Iteration: 70  train RMSE: 0.8867276167954357 train MAE: 0.7025052563946765\n",
      "Iteration: 71  train RMSE: 0.8858401958317237 train MAE: 0.6984715553983982\n",
      "Iteration: 72  train RMSE: 0.8852948631746977 train MAE: 0.7012917066436896\n",
      "Iteration: 73  train RMSE: 0.8844393174460841 train MAE: 0.6972530326675531\n",
      "Iteration: 74  train RMSE: 0.8839266742752534 train MAE: 0.7001190333734129\n",
      "Iteration: 75  train RMSE: 0.8830965714865414 train MAE: 0.6960967236642521\n",
      "Final train RMSE for fold: 2 : 0.8830965714865414\n",
      "Final train MAE for fold: 2 : 0.6960967236642521\n",
      "Final test RMSE for fold: 2 : 0.9253684595131506\n",
      "Final test MAE for fold: 2 : 0.7288461221150584\n",
      "Iteration: 1  train RMSE: 1.6794923983757988 train MAE: 1.4143234181142337\n",
      "Iteration: 2  train RMSE: 1.6492931759996896 train MAE: 1.3856676246181443\n",
      "Iteration: 3  train RMSE: 1.6194435153893885 train MAE: 1.3574877458481016\n",
      "Iteration: 4  train RMSE: 1.5899827311372676 train MAE: 1.329808651858354\n",
      "Iteration: 5  train RMSE: 1.5609488379346748 train MAE: 1.302671200304142\n",
      "Iteration: 6  train RMSE: 1.5323781971006039 train MAE: 1.2760999085548885\n",
      "Iteration: 7  train RMSE: 1.5043097422077496 train MAE: 1.2501401677604236\n",
      "Iteration: 8  train RMSE: 1.4767786338775621 train MAE: 1.2248086100007696\n",
      "Iteration: 9  train RMSE: 1.4498145928893778 train MAE: 1.2001184384255343\n",
      "Iteration: 10  train RMSE: 1.4234443250008486 train MAE: 1.176074535720045\n",
      "Iteration: 11  train RMSE: 1.3976930610146054 train MAE: 1.1526848475829639\n",
      "Iteration: 12  train RMSE: 1.3725911315959172 train MAE: 1.1299671080235367\n",
      "Iteration: 13  train RMSE: 1.3481683185824989 train MAE: 1.1079500654242247\n",
      "Iteration: 14  train RMSE: 1.3244458369122611 train MAE: 1.0866364043629484\n",
      "Iteration: 15  train RMSE: 1.3014459401359928 train MAE: 1.0660645949385148\n",
      "Iteration: 16  train RMSE: 1.2791758596506095 train MAE: 1.0462245393837015\n",
      "Iteration: 17  train RMSE: 1.2576415324925914 train MAE: 1.0271087742282874\n",
      "Iteration: 18  train RMSE: 1.2368494200823148 train MAE: 1.0086953956099212\n",
      "Iteration: 19  train RMSE: 1.2168082757656988 train MAE: 0.9909827332484792\n",
      "Iteration: 20  train RMSE: 1.1975246270907578 train MAE: 0.9739920218999634\n",
      "Iteration: 21  train RMSE: 1.1790020746247634 train MAE: 0.9577268577505217\n",
      "Iteration: 22  train RMSE: 1.1612415392170992 train MAE: 0.942156939912678\n",
      "Iteration: 23  train RMSE: 1.1442357307159807 train MAE: 0.9272983882814905\n",
      "Iteration: 24  train RMSE: 1.1279753318907055 train MAE: 0.9131007088604812\n",
      "Iteration: 25  train RMSE: 1.112459488353288 train MAE: 0.8996219350049973\n",
      "Iteration: 26  train RMSE: 1.0976675607623243 train MAE: 0.8867259810558175\n",
      "Iteration: 27  train RMSE: 1.083596339822903 train MAE: 0.8745309775951338\n",
      "Iteration: 28  train RMSE: 1.0702274328547499 train MAE: 0.8628495324436971\n",
      "Iteration: 29  train RMSE: 1.0575665706187645 train MAE: 0.8519468105077098\n",
      "Iteration: 30  train RMSE: 1.0455719271391042 train MAE: 0.8414111068554933\n",
      "Iteration: 31  train RMSE: 1.0342531391993772 train MAE: 0.8317306019716788\n",
      "Iteration: 32  train RMSE: 1.0235490671718124 train MAE: 0.8222118925017933\n",
      "Iteration: 33  train RMSE: 1.0134856885203103 train MAE: 0.8136842302975489\n",
      "Iteration: 34  train RMSE: 1.003990239051194 train MAE: 0.8050945633619289\n",
      "Iteration: 35  train RMSE: 0.9951103802813585 train MAE: 0.7976619478101117\n",
      "Iteration: 36  train RMSE: 0.9867584371592023 train MAE: 0.7899443335790803\n",
      "Iteration: 37  train RMSE: 0.9789816769625257 train MAE: 0.7836134801089033\n",
      "Iteration: 38  train RMSE: 0.9716591313595516 train MAE: 0.7766454130563758\n",
      "Iteration: 39  train RMSE: 0.9648710310936963 train MAE: 0.7712523525381134\n",
      "Iteration: 40  train RMSE: 0.9584853871487425 train MAE: 0.7649724495565835\n",
      "Iteration: 41  train RMSE: 0.9526177689772491 train MAE: 0.7605904278282855\n",
      "Iteration: 42  train RMSE: 0.9470790226535084 train MAE: 0.7548174035298314\n",
      "Iteration: 43  train RMSE: 0.9420213265933357 train MAE: 0.7513210629530722\n",
      "Iteration: 44  train RMSE: 0.9372280173183515 train MAE: 0.7460071021895336\n",
      "Iteration: 45  train RMSE: 0.9328918113356864 train MAE: 0.7433536527702823\n",
      "Iteration: 46  train RMSE: 0.9287488783244533 train MAE: 0.7383666271590208\n",
      "Iteration: 47  train RMSE: 0.925041657944678 train MAE: 0.7364542192636383\n",
      "Iteration: 48  train RMSE: 0.921486344557929 train MAE: 0.7316827997082932\n",
      "Iteration: 49  train RMSE: 0.9183606494887143 train MAE: 0.7305639983564918\n",
      "Iteration: 50  train RMSE: 0.9153090429895953 train MAE: 0.7259555416119203\n",
      "Iteration: 51  train RMSE: 0.9126750863191163 train MAE: 0.7255117114679279\n",
      "Iteration: 52  train RMSE: 0.9100539992795816 train MAE: 0.7210228753074168\n",
      "Iteration: 53  train RMSE: 0.9078632459409955 train MAE: 0.7212284367720004\n",
      "Iteration: 54  train RMSE: 0.90562549171202 train MAE: 0.7168629706839446\n",
      "Iteration: 55  train RMSE: 0.9038020256516637 train MAE: 0.7176388618157109\n",
      "Iteration: 56  train RMSE: 0.9018693146553616 train MAE: 0.7132921560194938\n",
      "Iteration: 57  train RMSE: 0.9003499513458253 train MAE: 0.7145257267645744\n",
      "Iteration: 58  train RMSE: 0.8986858818195714 train MAE: 0.7102623525578106\n",
      "Iteration: 59  train RMSE: 0.8974191944574855 train MAE: 0.7118773231269871\n",
      "Iteration: 60  train RMSE: 0.8959631405122211 train MAE: 0.7076533185184899\n",
      "Iteration: 61  train RMSE: 0.8949113871028032 train MAE: 0.7096361803384672\n",
      "Iteration: 62  train RMSE: 0.8936276691748914 train MAE: 0.7054594931927961\n",
      "Iteration: 63  train RMSE: 0.8927277291244916 train MAE: 0.7076881877884353\n",
      "Iteration: 64  train RMSE: 0.8915724009602998 train MAE: 0.703528091382193\n",
      "Iteration: 65  train RMSE: 0.8908022785515973 train MAE: 0.7059631733265764\n",
      "Iteration: 66  train RMSE: 0.8897584982609721 train MAE: 0.7018237594056757\n",
      "Iteration: 67  train RMSE: 0.8890856708346245 train MAE: 0.7044290849335445\n",
      "Iteration: 68  train RMSE: 0.8881177192100279 train MAE: 0.7003449707861095\n",
      "Iteration: 69  train RMSE: 0.8875289954007792 train MAE: 0.7030715584119629\n",
      "Iteration: 70  train RMSE: 0.8866294754860939 train MAE: 0.6989980963210873\n",
      "Iteration: 71  train RMSE: 0.8860999011464393 train MAE: 0.7018285008727689\n",
      "Iteration: 72  train RMSE: 0.8852334611120563 train MAE: 0.6977863444265854\n",
      "Iteration: 73  train RMSE: 0.8847446737938643 train MAE: 0.7006595481709534\n",
      "Iteration: 74  train RMSE: 0.8839088163673675 train MAE: 0.6966355641329216\n",
      "Iteration: 75  train RMSE: 0.8834428869155111 train MAE: 0.699527763781235\n",
      "Final train RMSE for fold: 3 : 0.8834428869155111\n",
      "Final train MAE for fold: 3 : 0.699527763781235\n",
      "Final test RMSE for fold: 3 : 0.9257082926737727\n",
      "Final test MAE for fold: 3 : 0.7327775176510971\n",
      "Iteration: 1  train RMSE: 1.6688695359514107 train MAE: 1.4033356095702512\n",
      "Iteration: 2  train RMSE: 1.638587569233116 train MAE: 1.3745688592575491\n",
      "Iteration: 3  train RMSE: 1.6086867414708852 train MAE: 1.346308395906302\n",
      "Iteration: 4  train RMSE: 1.5792153099222903 train MAE: 1.3186085731625499\n",
      "Iteration: 5  train RMSE: 1.5502136752500444 train MAE: 1.291500809218043\n",
      "Iteration: 6  train RMSE: 1.5217208219529106 train MAE: 1.2649984091895965\n",
      "Iteration: 7  train RMSE: 1.4937681227317525 train MAE: 1.239132384183165\n",
      "Iteration: 8  train RMSE: 1.4663897325311868 train MAE: 1.213933826077495\n",
      "Iteration: 9  train RMSE: 1.4396150138192652 train MAE: 1.1894063231022187\n",
      "Iteration: 10  train RMSE: 1.4134759426672525 train MAE: 1.1655714204699748\n",
      "Iteration: 11  train RMSE: 1.3879945897639414 train MAE: 1.1424379853028748\n",
      "Iteration: 12  train RMSE: 1.3631964733124633 train MAE: 1.1200300833695633\n",
      "Iteration: 13  train RMSE: 1.3390995843925864 train MAE: 1.0983569305944583\n",
      "Iteration: 14  train RMSE: 1.3157246398536846 train MAE: 1.0774270224686902\n",
      "Iteration: 15  train RMSE: 1.2930902707831622 train MAE: 1.0572521660438141\n",
      "Iteration: 16  train RMSE: 1.2712149676132134 train MAE: 1.0378282753699617\n",
      "Iteration: 17  train RMSE: 1.250110990370024 train MAE: 1.0191539828356428\n",
      "Iteration: 18  train RMSE: 1.2297795265297522 train MAE: 1.0012342310083129\n",
      "Iteration: 19  train RMSE: 1.2102179575339265 train MAE: 0.9840584384150038\n",
      "Iteration: 20  train RMSE: 1.1914240118479582 train MAE: 0.9676096884862475\n",
      "Iteration: 21  train RMSE: 1.1733914390867108 train MAE: 0.9518724266534884\n",
      "Iteration: 22  train RMSE: 1.1561171766708438 train MAE: 0.9368480614557895\n",
      "Iteration: 23  train RMSE: 1.1395922215482035 train MAE: 0.9225118588015595\n",
      "Iteration: 24  train RMSE: 1.123812824206175 train MAE: 0.9088454671363222\n",
      "Iteration: 25  train RMSE: 1.1087532450870063 train MAE: 0.8958194269160561\n",
      "Iteration: 26  train RMSE: 1.094408764875714 train MAE: 0.8833853964189041\n",
      "Iteration: 27  train RMSE: 1.0807693914355216 train MAE: 0.8716047042629341\n",
      "Iteration: 28  train RMSE: 1.0678185668565094 train MAE: 0.8603667768527858\n",
      "Iteration: 29  train RMSE: 1.0555418833732344 train MAE: 0.8497903533048572\n",
      "Iteration: 30  train RMSE: 1.0439153349695693 train MAE: 0.8396765729031522\n",
      "Iteration: 31  train RMSE: 1.0329360070009606 train MAE: 0.8302232881281139\n",
      "Iteration: 32  train RMSE: 1.0225678526441573 train MAE: 0.8211191097923864\n",
      "Iteration: 33  train RMSE: 1.0128134173337568 train MAE: 0.8127812599062876\n",
      "Iteration: 34  train RMSE: 1.0036148774894542 train MAE: 0.8045537302961154\n",
      "Iteration: 35  train RMSE: 0.9950004704740211 train MAE: 0.7973199052962764\n",
      "Iteration: 36  train RMSE: 0.986885642547594 train MAE: 0.7898754945530352\n",
      "Iteration: 37  train RMSE: 0.9793291017128685 train MAE: 0.7837356387848582\n",
      "Iteration: 38  train RMSE: 0.972198426918365 train MAE: 0.7769201003423286\n",
      "Iteration: 39  train RMSE: 0.9656033898906385 train MAE: 0.771778228247443\n",
      "Iteration: 40  train RMSE: 0.9593643776967928 train MAE: 0.7655781862535754\n",
      "Iteration: 41  train RMSE: 0.9536289702888604 train MAE: 0.7613232401854605\n",
      "Iteration: 42  train RMSE: 0.9481895311840512 train MAE: 0.7556069176440791\n",
      "Iteration: 43  train RMSE: 0.9432296117077881 train MAE: 0.752255301766539\n",
      "Iteration: 44  train RMSE: 0.9385034503311117 train MAE: 0.7469422626138145\n",
      "Iteration: 45  train RMSE: 0.9342358005513802 train MAE: 0.7444240800050866\n",
      "Iteration: 46  train RMSE: 0.930153290592431 train MAE: 0.7394236566126882\n",
      "Iteration: 47  train RMSE: 0.9264997365170953 train MAE: 0.7377026558653643\n",
      "Iteration: 48  train RMSE: 0.9229648257360803 train MAE: 0.7329297467906715\n",
      "Iteration: 49  train RMSE: 0.9198373072863452 train MAE: 0.7319064269420961\n",
      "Iteration: 50  train RMSE: 0.9167772039313166 train MAE: 0.727289167691987\n",
      "Iteration: 51  train RMSE: 0.9141258757322397 train MAE: 0.7269399562345226\n",
      "Iteration: 52  train RMSE: 0.9114962687767756 train MAE: 0.7224052012135231\n",
      "Iteration: 53  train RMSE: 0.9092503143233771 train MAE: 0.722643406742792\n",
      "Iteration: 54  train RMSE: 0.9069826544492975 train MAE: 0.7181994506964203\n",
      "Iteration: 55  train RMSE: 0.9050868644974223 train MAE: 0.71896001145396\n",
      "Iteration: 56  train RMSE: 0.9031328675244437 train MAE: 0.714534459288027\n",
      "Iteration: 57  train RMSE: 0.9015329542737867 train MAE: 0.7157682218442137\n",
      "Iteration: 58  train RMSE: 0.8998353011552529 train MAE: 0.7114068266625055\n",
      "Iteration: 59  train RMSE: 0.8984760562997184 train MAE: 0.7129879219809042\n",
      "Iteration: 60  train RMSE: 0.8969984240902742 train MAE: 0.7086862276616988\n",
      "Iteration: 61  train RMSE: 0.8958373564751523 train MAE: 0.7105998915463273\n",
      "Iteration: 62  train RMSE: 0.8945244978323293 train MAE: 0.7063456590455219\n",
      "Iteration: 63  train RMSE: 0.8935195545623024 train MAE: 0.708468596213943\n",
      "Iteration: 64  train RMSE: 0.8923451203729205 train MAE: 0.7042710242608753\n",
      "Iteration: 65  train RMSE: 0.8914810592574546 train MAE: 0.7065981892648515\n",
      "Iteration: 66  train RMSE: 0.8904214450808937 train MAE: 0.7024421376193315\n",
      "Iteration: 67  train RMSE: 0.889686434678557 train MAE: 0.7049727134965457\n",
      "Iteration: 68  train RMSE: 0.8887168406854113 train MAE: 0.7008609954779562\n",
      "Iteration: 69  train RMSE: 0.8880800823507828 train MAE: 0.7035347051734291\n",
      "Iteration: 70  train RMSE: 0.8871834085787246 train MAE: 0.6994584148972851\n",
      "Iteration: 71  train RMSE: 0.8866335999072785 train MAE: 0.7022906965670412\n",
      "Iteration: 72  train RMSE: 0.8857839394657931 train MAE: 0.6982262447067721\n",
      "Iteration: 73  train RMSE: 0.8852934951042762 train MAE: 0.7011388368967664\n",
      "Iteration: 74  train RMSE: 0.8844759632290128 train MAE: 0.6970847727591271\n",
      "Iteration: 75  train RMSE: 0.8840193064915812 train MAE: 0.700024795407708\n",
      "Final train RMSE for fold: 4 : 0.8840193064915812\n",
      "Final train MAE for fold: 4 : 0.700024795407708\n",
      "Final test RMSE for fold: 4 : 0.9247271439918301\n",
      "Final test MAE for fold: 4 : 0.7319982864333237\n",
      "Iteration: 1  train RMSE: 1.6813258829807214 train MAE: 1.4144844640458507\n",
      "Iteration: 2  train RMSE: 1.6511570066416736 train MAE: 1.3858030340104575\n",
      "Iteration: 3  train RMSE: 1.6213499817366623 train MAE: 1.3576186582341796\n",
      "Iteration: 4  train RMSE: 1.5919547125431461 train MAE: 1.3299615234536677\n",
      "Iteration: 5  train RMSE: 1.5630068482869333 train MAE: 1.3028758749766467\n",
      "Iteration: 6  train RMSE: 1.5345421742827434 train MAE: 1.2763907048479333\n",
      "Iteration: 7  train RMSE: 1.5066058430149876 train MAE: 1.2505464084294033\n",
      "Iteration: 8  train RMSE: 1.4792159228845214 train MAE: 1.2253315261450948\n",
      "Iteration: 9  train RMSE: 1.4523995255316113 train MAE: 1.2007775480776492\n",
      "Iteration: 10  train RMSE: 1.4261839260805402 train MAE: 1.1768843302843344\n",
      "Iteration: 11  train RMSE: 1.400598811946555 train MAE: 1.1536723498763588\n",
      "Iteration: 12  train RMSE: 1.3756655236017403 train MAE: 1.13114055888526\n",
      "Iteration: 13  train RMSE: 1.351403546202925 train MAE: 1.1093180904023892\n",
      "Iteration: 14  train RMSE: 1.327835504545014 train MAE: 1.0882265350321754\n",
      "Iteration: 15  train RMSE: 1.304970798441308 train MAE: 1.0678518931836167\n",
      "Iteration: 16  train RMSE: 1.2828234373111693 train MAE: 1.0481924590616862\n",
      "Iteration: 17  train RMSE: 1.2614022811056371 train MAE: 1.0292495665560535\n",
      "Iteration: 18  train RMSE: 1.2407195683604766 train MAE: 1.0110169864285379\n",
      "Iteration: 19  train RMSE: 1.220776031398986 train MAE: 0.9934891816231238\n",
      "Iteration: 20  train RMSE: 1.201573452061962 train MAE: 0.9766766284476158\n",
      "Iteration: 21  train RMSE: 1.1831157868416655 train MAE: 0.960565716054624\n",
      "Iteration: 22  train RMSE: 1.1654072598239829 train MAE: 0.9451361275067132\n",
      "Iteration: 23  train RMSE: 1.148443525441547 train MAE: 0.9303974300997284\n",
      "Iteration: 24  train RMSE: 1.1322270377655537 train MAE: 0.9163229153617876\n",
      "Iteration: 25  train RMSE: 1.1167483755530239 train MAE: 0.9029280535269223\n",
      "Iteration: 26  train RMSE: 1.1019937155424593 train MAE: 0.8901493008501044\n",
      "Iteration: 27  train RMSE: 1.0879526634630112 train MAE: 0.8780328344825888\n",
      "Iteration: 28  train RMSE: 1.07461010409024 train MAE: 0.8664627298170938\n",
      "Iteration: 29  train RMSE: 1.0619583521311642 train MAE: 0.8555792351204385\n",
      "Iteration: 30  train RMSE: 1.049965695121212 train MAE: 0.8451199723942945\n",
      "Iteration: 31  train RMSE: 1.0386402705070206 train MAE: 0.8354619093611827\n",
      "Iteration: 32  train RMSE: 1.027933126609228 train MAE: 0.8260298892176928\n",
      "Iteration: 33  train RMSE: 1.0178575903460638 train MAE: 0.817517304052216\n",
      "Iteration: 34  train RMSE: 1.008339599919514 train MAE: 0.8089810042393458\n",
      "Iteration: 35  train RMSE: 0.9994112936164615 train MAE: 0.8015234829709564\n",
      "Iteration: 36  train RMSE: 0.9909859014018757 train MAE: 0.7937805198229012\n",
      "Iteration: 37  train RMSE: 0.9831210897357598 train MAE: 0.7873401049149658\n",
      "Iteration: 38  train RMSE: 0.9757098185976507 train MAE: 0.7802910686036232\n",
      "Iteration: 39  train RMSE: 0.9688194319017976 train MAE: 0.774790917551189\n",
      "Iteration: 40  train RMSE: 0.9623334501961986 train MAE: 0.7683100422230993\n",
      "Iteration: 41  train RMSE: 0.9563536498900628 train MAE: 0.7637525293870662\n",
      "Iteration: 42  train RMSE: 0.9507191589772307 train MAE: 0.757793143881724\n",
      "Iteration: 43  train RMSE: 0.9455659288732051 train MAE: 0.7541619254800184\n",
      "Iteration: 44  train RMSE: 0.94070430775383 train MAE: 0.7486938196989472\n",
      "Iteration: 45  train RMSE: 0.936285028526013 train MAE: 0.7459651977282762\n",
      "Iteration: 46  train RMSE: 0.9320920624354702 train MAE: 0.7408646276306717\n",
      "Iteration: 47  train RMSE: 0.9283283269878201 train MAE: 0.7389817078072949\n",
      "Iteration: 48  train RMSE: 0.9247304268749501 train MAE: 0.7341140704672948\n",
      "Iteration: 49  train RMSE: 0.9215291025806005 train MAE: 0.7329840517274825\n",
      "Iteration: 50  train RMSE: 0.9184378343887909 train MAE: 0.7283521747967721\n",
      "Iteration: 51  train RMSE: 0.9157197830982442 train MAE: 0.7278319473564376\n",
      "Iteration: 52  train RMSE: 0.9130797512340076 train MAE: 0.7233786283146769\n",
      "Iteration: 53  train RMSE: 0.9108000773264132 train MAE: 0.7234843548139439\n",
      "Iteration: 54  train RMSE: 0.9085460367920349 train MAE: 0.7191301135315844\n",
      "Iteration: 55  train RMSE: 0.9066352532472913 train MAE: 0.7198438278285489\n",
      "Iteration: 56  train RMSE: 0.9046975307240623 train MAE: 0.7155585664876721\n",
      "Iteration: 57  train RMSE: 0.903089526619606 train MAE: 0.7167732868264312\n",
      "Iteration: 58  train RMSE: 0.9014021556374686 train MAE: 0.7124985705819606\n",
      "Iteration: 59  train RMSE: 0.9000355674334882 train MAE: 0.714133788770439\n",
      "Iteration: 60  train RMSE: 0.8985475007085952 train MAE: 0.70985357568834\n",
      "Iteration: 61  train RMSE: 0.8973791139784026 train MAE: 0.7118073004492318\n",
      "Iteration: 62  train RMSE: 0.8960595547819024 train MAE: 0.7075464266261153\n",
      "Iteration: 63  train RMSE: 0.8950480648652107 train MAE: 0.7097437035388692\n",
      "Iteration: 64  train RMSE: 0.8938621663563123 train MAE: 0.7055117601102516\n",
      "Iteration: 65  train RMSE: 0.892994384437886 train MAE: 0.7079491092680373\n",
      "Iteration: 66  train RMSE: 0.8919237302338534 train MAE: 0.7037509730125348\n",
      "Iteration: 67  train RMSE: 0.891158280341759 train MAE: 0.7063721149196737\n",
      "Iteration: 68  train RMSE: 0.8901666145839887 train MAE: 0.7021641102933468\n",
      "Iteration: 69  train RMSE: 0.8894819017218841 train MAE: 0.7049179872839311\n",
      "Iteration: 70  train RMSE: 0.888556610818175 train MAE: 0.7007270897730761\n",
      "Iteration: 71  train RMSE: 0.8879382951678685 train MAE: 0.7035860083688458\n",
      "Iteration: 72  train RMSE: 0.8870646390659104 train MAE: 0.6994130473262765\n",
      "Iteration: 73  train RMSE: 0.8864991865253212 train MAE: 0.7023574971253042\n",
      "Iteration: 74  train RMSE: 0.885662335830143 train MAE: 0.6982009049612593\n",
      "Iteration: 75  train RMSE: 0.8851326956588628 train MAE: 0.7011843716316949\n",
      "Final train RMSE for fold: 5 : 0.8851326956588628\n",
      "Final train MAE for fold: 5 : 0.7011843716316949\n",
      "Final test RMSE for fold: 5 : 0.9269397617109386\n",
      "Final test MAE for fold: 5 : 0.7336067461007537\n",
      "Overall Train RMSE: 0.8839608355048773\n",
      "Overall Train MAE: 0.6994310596144893\n",
      "Overall Test RMSE: 0.9253240684611157\n",
      "Overall Test MAE: 0.7316915551664489\n"
     ]
    }
   ],
   "source": [
    "model1=MatrixFactorization()\n",
    "(plot1_train,plot1_test)=cross_validation(model1, data_df, num_folds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71464d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  train RMSE: 1.3453559404161008 train MAE: 1.0770126138689624\n",
      "Iteration: 2  train RMSE: 1.32039543990401 train MAE: 1.0560947613667626\n",
      "Iteration: 3  train RMSE: 1.2960030970099228 train MAE: 1.0357215511899154\n",
      "Iteration: 4  train RMSE: 1.2722463956755972 train MAE: 1.015902835141665\n",
      "Iteration: 5  train RMSE: 1.249182908097547 train MAE: 0.9966689711931591\n",
      "Iteration: 6  train RMSE: 1.2268901124617844 train MAE: 0.9781283622528991\n",
      "Iteration: 7  train RMSE: 1.2054203327196817 train MAE: 0.9603257278398791\n",
      "Iteration: 8  train RMSE: 1.1847990477101216 train MAE: 0.9432601542019601\n",
      "Iteration: 9  train RMSE: 1.1650584435747835 train MAE: 0.9269735809170294\n",
      "Iteration: 10  train RMSE: 1.1462091274139234 train MAE: 0.9114550420897843\n",
      "Iteration: 11  train RMSE: 1.1282448199348882 train MAE: 0.8967254542277437\n",
      "Iteration: 12  train RMSE: 1.1111736945531943 train MAE: 0.8827723007761215\n",
      "Iteration: 13  train RMSE: 1.0949889079851507 train MAE: 0.8695611064521335\n",
      "Iteration: 14  train RMSE: 1.0796990366713926 train MAE: 0.8570855178626771\n",
      "Iteration: 15  train RMSE: 1.065274294767175 train MAE: 0.8453170641291907\n",
      "Iteration: 16  train RMSE: 1.051701216551372 train MAE: 0.8342345409763505\n",
      "Iteration: 17  train RMSE: 1.0389607716321068 train MAE: 0.8238902726563726\n",
      "Iteration: 18  train RMSE: 1.0270213206287777 train MAE: 0.8140820559046713\n",
      "Iteration: 19  train RMSE: 1.0158750160461556 train MAE: 0.8050832892392161\n",
      "Iteration: 20  train RMSE: 1.005470241649984 train MAE: 0.7965160035131607\n",
      "Iteration: 21  train RMSE: 0.9957866291546315 train MAE: 0.7887741418940606\n",
      "Iteration: 22  train RMSE: 0.9867505739056144 train MAE: 0.7811872739732031\n",
      "Iteration: 23  train RMSE: 0.9783851920994086 train MAE: 0.7747575902814178\n",
      "Iteration: 24  train RMSE: 0.9705741107429587 train MAE: 0.767917995856059\n",
      "Iteration: 25  train RMSE: 0.9634078917462078 train MAE: 0.762856016811053\n",
      "Iteration: 26  train RMSE: 0.9566843933977657 train MAE: 0.7564450679209339\n",
      "Iteration: 27  train RMSE: 0.9505966495847599 train MAE: 0.7527366067630532\n",
      "Iteration: 28  train RMSE: 0.94483759425575 train MAE: 0.7466033188209632\n",
      "Iteration: 29  train RMSE: 0.9396875302736195 train MAE: 0.7441098330614367\n",
      "Iteration: 30  train RMSE: 0.9347577065369833 train MAE: 0.7382006617762317\n",
      "Iteration: 31  train RMSE: 0.9304220823832642 train MAE: 0.7367577735213954\n",
      "Iteration: 32  train RMSE: 0.9262047029650177 train MAE: 0.731074837179739\n",
      "Iteration: 33  train RMSE: 0.9225908258195185 train MAE: 0.7305948746039993\n",
      "Iteration: 34  train RMSE: 0.9189849743841547 train MAE: 0.7250594608683135\n",
      "Iteration: 35  train RMSE: 0.915987823451312 train MAE: 0.725429443767173\n",
      "Iteration: 36  train RMSE: 0.9128964717505332 train MAE: 0.7199873401152103\n",
      "Iteration: 37  train RMSE: 0.9104416703847373 train MAE: 0.7211725916900037\n",
      "Iteration: 38  train RMSE: 0.9077730831788325 train MAE: 0.7157174491740209\n",
      "Iteration: 39  train RMSE: 0.9057611973660934 train MAE: 0.717592261520769\n",
      "Iteration: 40  train RMSE: 0.9034232066405192 train MAE: 0.7121422962934612\n",
      "Iteration: 41  train RMSE: 0.9017707005478538 train MAE: 0.7145474697182114\n",
      "Iteration: 42  train RMSE: 0.8997017494350427 train MAE: 0.7091238373985672\n",
      "Iteration: 43  train RMSE: 0.8983295891949891 train MAE: 0.7119183324106964\n",
      "Iteration: 44  train RMSE: 0.896462063983191 train MAE: 0.7065006242041809\n",
      "Iteration: 45  train RMSE: 0.8953264779052452 train MAE: 0.7096193169322788\n",
      "Iteration: 46  train RMSE: 0.8936320021154505 train MAE: 0.7041951155118334\n",
      "Iteration: 47  train RMSE: 0.892694199120892 train MAE: 0.7075783128527549\n",
      "Iteration: 48  train RMSE: 0.8911272932516654 train MAE: 0.7021717458181623\n",
      "Iteration: 49  train RMSE: 0.890340377610344 train MAE: 0.7057525195232863\n",
      "Iteration: 50  train RMSE: 0.8888862041191582 train MAE: 0.7003532968104322\n",
      "Iteration: 51  train RMSE: 0.8882208000664867 train MAE: 0.7040732253622443\n",
      "Iteration: 52  train RMSE: 0.886851511475814 train MAE: 0.6986851872396752\n",
      "Iteration: 53  train RMSE: 0.8862714631201374 train MAE: 0.7024998165959868\n",
      "Iteration: 54  train RMSE: 0.8849771308360929 train MAE: 0.6971480128793776\n",
      "Iteration: 55  train RMSE: 0.8844683036927524 train MAE: 0.7010450253639473\n",
      "Iteration: 56  train RMSE: 0.8832391251218639 train MAE: 0.6957104847459396\n",
      "Iteration: 57  train RMSE: 0.8827914947704733 train MAE: 0.6996878772322682\n",
      "Iteration: 58  train RMSE: 0.881605665824104 train MAE: 0.6943727773154592\n",
      "Iteration: 59  train RMSE: 0.8811944566676211 train MAE: 0.6983809760371755\n",
      "Iteration: 60  train RMSE: 0.8800476061798708 train MAE: 0.6930835854486371\n",
      "Iteration: 61  train RMSE: 0.8796685007168227 train MAE: 0.6971331298275659\n",
      "Iteration: 62  train RMSE: 0.8785502435005279 train MAE: 0.6918557536881349\n",
      "Iteration: 63  train RMSE: 0.878184404217598 train MAE: 0.6959189015540309\n",
      "Iteration: 64  train RMSE: 0.8770875883293041 train MAE: 0.6906584233256704\n",
      "Iteration: 65  train RMSE: 0.8767348625394719 train MAE: 0.6947306745984294\n",
      "Iteration: 66  train RMSE: 0.8756518174327519 train MAE: 0.6894863785105082\n",
      "Iteration: 67  train RMSE: 0.8753036381336726 train MAE: 0.6935520697044754\n",
      "Iteration: 68  train RMSE: 0.874230714358642 train MAE: 0.6883201907997301\n",
      "Iteration: 69  train RMSE: 0.8738817393081161 train MAE: 0.6923791917161094\n",
      "Iteration: 70  train RMSE: 0.8728111423049583 train MAE: 0.6871648672436277\n",
      "Iteration: 71  train RMSE: 0.8724570225118755 train MAE: 0.6912020484528747\n",
      "Iteration: 72  train RMSE: 0.8713896171424509 train MAE: 0.6860092643219554\n",
      "Iteration: 73  train RMSE: 0.8710304204597824 train MAE: 0.6900185641292963\n",
      "Iteration: 74  train RMSE: 0.8699622629734597 train MAE: 0.6848431273160779\n",
      "Iteration: 75  train RMSE: 0.8695881077565408 train MAE: 0.6888179868556307\n",
      "Final train RMSE for fold: 1 : 0.8695881077565408\n",
      "Final train MAE for fold: 1 : 0.6888179868556307\n",
      "Final test RMSE for fold: 1 : 0.9318234273198384\n",
      "Final test MAE for fold: 1 : 0.737301587129596\n",
      "Iteration: 1  train RMSE: 1.3512513675163385 train MAE: 1.0782240974225152\n",
      "Iteration: 2  train RMSE: 1.3259666045112612 train MAE: 1.05705246763167\n",
      "Iteration: 3  train RMSE: 1.301254962917425 train MAE: 1.0364470197709672\n",
      "Iteration: 4  train RMSE: 1.2771865976644805 train MAE: 1.0164623427641877\n",
      "Iteration: 5  train RMSE: 1.2538473720022085 train MAE: 0.9971831562392436\n",
      "Iteration: 6  train RMSE: 1.2313022253230672 train MAE: 0.9786436495841007\n",
      "Iteration: 7  train RMSE: 1.209602625306254 train MAE: 0.9608731338917066\n",
      "Iteration: 8  train RMSE: 1.1887808897874161 train MAE: 0.9438590711444355\n",
      "Iteration: 9  train RMSE: 1.1688669535423848 train MAE: 0.9276338596226497\n",
      "Iteration: 10  train RMSE: 1.1498752904447516 train MAE: 0.9122026428062993\n",
      "Iteration: 11  train RMSE: 1.1318129199261215 train MAE: 0.8975394506641595\n",
      "Iteration: 12  train RMSE: 1.1146802812971541 train MAE: 0.8836509701497569\n",
      "Iteration: 13  train RMSE: 1.0984619323783673 train MAE: 0.8704971020076452\n",
      "Iteration: 14  train RMSE: 1.0831515844439707 train MAE: 0.858087544874585\n",
      "Iteration: 15  train RMSE: 1.0687255341513693 train MAE: 0.846396236280026\n",
      "Iteration: 16  train RMSE: 1.055154981649287 train MAE: 0.8354222495878233\n",
      "Iteration: 17  train RMSE: 1.0424122172787682 train MAE: 0.8251323988744551\n",
      "Iteration: 18  train RMSE: 1.0304796193544317 train MAE: 0.815505641243543\n",
      "Iteration: 19  train RMSE: 1.0193206037386373 train MAE: 0.8064982433162604\n",
      "Iteration: 20  train RMSE: 1.008918030786906 train MAE: 0.798165828963389\n",
      "Iteration: 21  train RMSE: 0.9992015333626122 train MAE: 0.7903040069161826\n",
      "Iteration: 22  train RMSE: 0.9901614320169163 train MAE: 0.7831578846004404\n",
      "Iteration: 23  train RMSE: 0.981719985094321 train MAE: 0.7762489842696297\n",
      "Iteration: 24  train RMSE: 0.9738961012895038 train MAE: 0.7702291465241028\n",
      "Iteration: 25  train RMSE: 0.9665913703197309 train MAE: 0.7640217997778935\n",
      "Iteration: 26  train RMSE: 0.9598598857693952 train MAE: 0.7592170465386873\n",
      "Iteration: 27  train RMSE: 0.9535494946972308 train MAE: 0.7533450776069731\n",
      "Iteration: 28  train RMSE: 0.9478037116608706 train MAE: 0.7499239798092167\n",
      "Iteration: 29  train RMSE: 0.94238414029726 train MAE: 0.7442222978988544\n",
      "Iteration: 30  train RMSE: 0.93752021988223 train MAE: 0.7421227753298512\n",
      "Iteration: 31  train RMSE: 0.9328647080871714 train MAE: 0.7364440331877463\n",
      "Iteration: 32  train RMSE: 0.928755389664896 train MAE: 0.7353927012892554\n",
      "Iteration: 33  train RMSE: 0.9247386990821936 train MAE: 0.7298466870206693\n",
      "Iteration: 34  train RMSE: 0.9212823923781167 train MAE: 0.729705564450289\n",
      "Iteration: 35  train RMSE: 0.9178187833720479 train MAE: 0.7242047847408928\n",
      "Iteration: 36  train RMSE: 0.9149428035832398 train MAE: 0.7249137187819288\n",
      "Iteration: 37  train RMSE: 0.911935951961694 train MAE: 0.7194269596175138\n",
      "Iteration: 38  train RMSE: 0.909549035757089 train MAE: 0.7208306674199018\n",
      "Iteration: 39  train RMSE: 0.906920801773225 train MAE: 0.7153409190246679\n",
      "Iteration: 40  train RMSE: 0.9049554635884309 train MAE: 0.7173535595884364\n",
      "Iteration: 41  train RMSE: 0.9026270958168969 train MAE: 0.711843393498122\n",
      "Iteration: 42  train RMSE: 0.9009772943636454 train MAE: 0.714251538817588\n",
      "Iteration: 43  train RMSE: 0.898900537878783 train MAE: 0.7087680795319397\n",
      "Iteration: 44  train RMSE: 0.8975286166909304 train MAE: 0.7115702971906226\n",
      "Iteration: 45  train RMSE: 0.8956503310517658 train MAE: 0.70609511071217\n",
      "Iteration: 46  train RMSE: 0.8945026634874846 train MAE: 0.7092024438113262\n",
      "Iteration: 47  train RMSE: 0.8927848490213536 train MAE: 0.7037526538109697\n",
      "Iteration: 48  train RMSE: 0.89181694488553 train MAE: 0.7070798858295141\n",
      "Iteration: 49  train RMSE: 0.8902343764265691 train MAE: 0.7016597399812124\n",
      "Iteration: 50  train RMSE: 0.8894027934132945 train MAE: 0.7051481416985981\n",
      "Iteration: 51  train RMSE: 0.8879269195500803 train MAE: 0.699757505563829\n",
      "Iteration: 52  train RMSE: 0.8872227425749617 train MAE: 0.7034164075813794\n",
      "Iteration: 53  train RMSE: 0.8858413219028403 train MAE: 0.6980251016672372\n",
      "Iteration: 54  train RMSE: 0.8852316206009675 train MAE: 0.7018040357746249\n",
      "Iteration: 55  train RMSE: 0.8839198366001478 train MAE: 0.6964426844341028\n",
      "Iteration: 56  train RMSE: 0.883373454538518 train MAE: 0.7002912726532804\n",
      "Iteration: 57  train RMSE: 0.8821168014427168 train MAE: 0.69494872712058\n",
      "Iteration: 58  train RMSE: 0.8816256463040368 train MAE: 0.6988565662808017\n",
      "Iteration: 59  train RMSE: 0.8804190283661881 train MAE: 0.6935280653746803\n",
      "Iteration: 60  train RMSE: 0.879964261333746 train MAE: 0.6974758518738513\n",
      "Iteration: 61  train RMSE: 0.8788002069589227 train MAE: 0.6921713105329332\n",
      "Iteration: 62  train RMSE: 0.8783885254592749 train MAE: 0.6961772315598629\n",
      "Iteration: 63  train RMSE: 0.8772553810858318 train MAE: 0.6908780743519096\n",
      "Iteration: 64  train RMSE: 0.8768600615455632 train MAE: 0.6949129829693576\n",
      "Iteration: 65  train RMSE: 0.8757443537575499 train MAE: 0.6896373130154758\n",
      "Iteration: 66  train RMSE: 0.8753613347445447 train MAE: 0.6936751509101475\n",
      "Iteration: 67  train RMSE: 0.8742600011090708 train MAE: 0.6884219018108723\n",
      "Iteration: 68  train RMSE: 0.8738792863123973 train MAE: 0.6924379198681575\n",
      "Iteration: 69  train RMSE: 0.8727830052352137 train MAE: 0.6872127869013903\n",
      "Iteration: 70  train RMSE: 0.8723992596654393 train MAE: 0.6912109418759991\n",
      "Iteration: 71  train RMSE: 0.8713053069165932 train MAE: 0.6860106704588851\n",
      "Iteration: 72  train RMSE: 0.8709131018955464 train MAE: 0.6899754711566671\n",
      "Iteration: 73  train RMSE: 0.8698181810424692 train MAE: 0.6847944673096501\n",
      "Iteration: 74  train RMSE: 0.86941055431751 train MAE: 0.6887276059535851\n",
      "Iteration: 75  train RMSE: 0.868305874422749 train MAE: 0.6835749582342134\n",
      "Final train RMSE for fold: 2 : 0.868305874422749\n",
      "Final train MAE for fold: 2 : 0.6835749582342134\n",
      "Final test RMSE for fold: 2 : 0.9325294017743897\n",
      "Final test MAE for fold: 2 : 0.733536009835196\n",
      "Iteration: 1  train RMSE: 1.3553164595990101 train MAE: 1.085453469626141\n",
      "Iteration: 2  train RMSE: 1.3300815706258218 train MAE: 1.064349438367385\n",
      "Iteration: 3  train RMSE: 1.3054171938815862 train MAE: 1.0437706691621742\n",
      "Iteration: 4  train RMSE: 1.281371257612993 train MAE: 1.0237410417020723\n",
      "Iteration: 5  train RMSE: 1.2580347783659689 train MAE: 1.0043494792551142\n",
      "Iteration: 6  train RMSE: 1.2354546615539574 train MAE: 0.9856192331350757\n",
      "Iteration: 7  train RMSE: 1.213707910006961 train MAE: 0.9676332169029225\n",
      "Iteration: 8  train RMSE: 1.1928072830335565 train MAE: 0.9503804787610861\n",
      "Iteration: 9  train RMSE: 1.1727984079427456 train MAE: 0.9339392784069263\n",
      "Iteration: 10  train RMSE: 1.1536610083616259 train MAE: 0.9182095729204467\n",
      "Iteration: 11  train RMSE: 1.1354395641034882 train MAE: 0.903280952711758\n",
      "Iteration: 12  train RMSE: 1.118106905284902 train MAE: 0.8890604999481577\n",
      "Iteration: 13  train RMSE: 1.1016864296460875 train MAE: 0.8756799224770915\n",
      "Iteration: 14  train RMSE: 1.0861413293469073 train MAE: 0.86293640057903\n",
      "Iteration: 15  train RMSE: 1.0714662570625446 train MAE: 0.8509867061136536\n",
      "Iteration: 16  train RMSE: 1.0576312169357154 train MAE: 0.8396192958071561\n",
      "Iteration: 17  train RMSE: 1.04463535117292 train MAE: 0.8290892354606212\n",
      "Iteration: 18  train RMSE: 1.0324215934401555 train MAE: 0.8189348497128688\n",
      "Iteration: 19  train RMSE: 1.0209997198424465 train MAE: 0.8097976169683998\n",
      "Iteration: 20  train RMSE: 1.010281609516262 train MAE: 0.8007280239447887\n",
      "Iteration: 21  train RMSE: 1.0003202852095174 train MAE: 0.7929801484205359\n",
      "Iteration: 22  train RMSE: 0.9909785181748876 train MAE: 0.7848363591104671\n",
      "Iteration: 23  train RMSE: 0.9823360117786423 train MAE: 0.7783862867946504\n",
      "Iteration: 24  train RMSE: 0.9742258224343673 train MAE: 0.7710267467097579\n",
      "Iteration: 25  train RMSE: 0.9667644959926478 train MAE: 0.7658306742612444\n",
      "Iteration: 26  train RMSE: 0.9597405067748624 train MAE: 0.759053735030827\n",
      "Iteration: 27  train RMSE: 0.9533373159473777 train MAE: 0.7550619572904336\n",
      "Iteration: 28  train RMSE: 0.9472847978126174 train MAE: 0.7487258583785726\n",
      "Iteration: 29  train RMSE: 0.941825580205819 train MAE: 0.7459237327196253\n",
      "Iteration: 30  train RMSE: 0.9366067965066532 train MAE: 0.7398960974130597\n",
      "Iteration: 31  train RMSE: 0.9319723542496001 train MAE: 0.7381256634628872\n",
      "Iteration: 32  train RMSE: 0.9274964639058106 train MAE: 0.7323315500100364\n",
      "Iteration: 33  train RMSE: 0.9236078584364372 train MAE: 0.731554489102254\n",
      "Iteration: 34  train RMSE: 0.9197735714176627 train MAE: 0.7259149264780219\n",
      "Iteration: 35  train RMSE: 0.9165401726740788 train MAE: 0.726042222552975\n",
      "Iteration: 36  train RMSE: 0.9132480638255036 train MAE: 0.7204769555140796\n",
      "Iteration: 37  train RMSE: 0.9105784933057306 train MAE: 0.7213875725083058\n",
      "Iteration: 38  train RMSE: 0.9077492087015253 train MAE: 0.7158865742371794\n",
      "Iteration: 39  train RMSE: 0.9055595383105863 train MAE: 0.7174594263552252\n",
      "Iteration: 40  train RMSE: 0.9031068558624019 train MAE: 0.7120221842105996\n",
      "Iteration: 41  train RMSE: 0.9013322923309663 train MAE: 0.7141840941365423\n",
      "Iteration: 42  train RMSE: 0.8991784844827336 train MAE: 0.7087660247852596\n",
      "Iteration: 43  train RMSE: 0.8977406689635496 train MAE: 0.7114200631598653\n",
      "Iteration: 44  train RMSE: 0.89582336143408 train MAE: 0.706005311055301\n",
      "Iteration: 45  train RMSE: 0.8946458030421799 train MAE: 0.7090084593255974\n",
      "Iteration: 46  train RMSE: 0.8929117172703305 train MAE: 0.7036168351928505\n",
      "Iteration: 47  train RMSE: 0.8919590277145825 train MAE: 0.7069144958060664\n",
      "Iteration: 48  train RMSE: 0.8903820002641442 train MAE: 0.7015251351844715\n",
      "Iteration: 49  train RMSE: 0.8896063817908033 train MAE: 0.7050744734931093\n",
      "Iteration: 50  train RMSE: 0.8881449461946429 train MAE: 0.699700281993581\n",
      "Iteration: 51  train RMSE: 0.8874919285081372 train MAE: 0.7034027722688033\n",
      "Iteration: 52  train RMSE: 0.8861204676335497 train MAE: 0.6980451405943753\n",
      "Iteration: 53  train RMSE: 0.885549568288445 train MAE: 0.7018408642079687\n",
      "Iteration: 54  train RMSE: 0.8842584051187172 train MAE: 0.6965094076050009\n",
      "Iteration: 55  train RMSE: 0.8837555000860057 train MAE: 0.7003919084696498\n",
      "Iteration: 56  train RMSE: 0.8825246519700544 train MAE: 0.6950663303870189\n",
      "Iteration: 57  train RMSE: 0.8820746671434505 train MAE: 0.6990194965058019\n",
      "Iteration: 58  train RMSE: 0.8808926390915051 train MAE: 0.6937170179766976\n",
      "Iteration: 59  train RMSE: 0.8804774532800133 train MAE: 0.6977042754503452\n",
      "Iteration: 60  train RMSE: 0.8793346695738332 train MAE: 0.6924221047012509\n",
      "Iteration: 61  train RMSE: 0.8789486670174809 train MAE: 0.6964474361548174\n",
      "Iteration: 62  train RMSE: 0.8778365079968988 train MAE: 0.6911808764536591\n",
      "Iteration: 63  train RMSE: 0.8774715473240767 train MAE: 0.6952197797670978\n",
      "Iteration: 64  train RMSE: 0.8763816462995758 train MAE: 0.6899718783782163\n",
      "Iteration: 65  train RMSE: 0.8760312635404827 train MAE: 0.6940225588694546\n",
      "Iteration: 66  train RMSE: 0.8749588878108724 train MAE: 0.688793549782068\n",
      "Iteration: 67  train RMSE: 0.8746098779335213 train MAE: 0.6928339909053247\n",
      "Iteration: 68  train RMSE: 0.8735470135848498 train MAE: 0.6876343392386552\n",
      "Iteration: 69  train RMSE: 0.8732033881453466 train MAE: 0.6916725357270328\n",
      "Iteration: 70  train RMSE: 0.8721518384209196 train MAE: 0.6864854781016013\n",
      "Iteration: 71  train RMSE: 0.8718035335176509 train MAE: 0.6905102580972797\n",
      "Iteration: 72  train RMSE: 0.8707498546817376 train MAE: 0.685350134714204\n",
      "Iteration: 73  train RMSE: 0.8703947308816296 train MAE: 0.6893379516034253\n",
      "Iteration: 74  train RMSE: 0.8693418142847119 train MAE: 0.6841972898174937\n",
      "Iteration: 75  train RMSE: 0.8689772763167699 train MAE: 0.6881587175718625\n",
      "Final train RMSE for fold: 3 : 0.8689772763167699\n",
      "Final train MAE for fold: 3 : 0.6881587175718625\n",
      "Final test RMSE for fold: 3 : 0.9340008236991083\n",
      "Final test MAE for fold: 3 : 0.7394673244495822\n",
      "Iteration: 1  train RMSE: 1.3492045430425887 train MAE: 1.078141813344921\n",
      "Iteration: 2  train RMSE: 1.3242099742608948 train MAE: 1.0572394178147717\n",
      "Iteration: 3  train RMSE: 1.2997920856079983 train MAE: 1.0368843532271907\n",
      "Iteration: 4  train RMSE: 1.2760266245706 train MAE: 1.0170905635195435\n",
      "Iteration: 5  train RMSE: 1.252989318379589 train MAE: 0.9979494242494785\n",
      "Iteration: 6  train RMSE: 1.230711760470101 train MAE: 0.9795094025008775\n",
      "Iteration: 7  train RMSE: 1.2092394455085653 train MAE: 0.9618087740246821\n",
      "Iteration: 8  train RMSE: 1.1886079471921975 train MAE: 0.9448536547603357\n",
      "Iteration: 9  train RMSE: 1.168813554963763 train MAE: 0.9285940279096744\n",
      "Iteration: 10  train RMSE: 1.1498994388847168 train MAE: 0.9131533054022283\n",
      "Iteration: 11  train RMSE: 1.1318412402498501 train MAE: 0.8983808279701447\n",
      "Iteration: 12  train RMSE: 1.114688077397584 train MAE: 0.8844224551630507\n",
      "Iteration: 13  train RMSE: 1.0984109402512239 train MAE: 0.8711497943590853\n",
      "Iteration: 14  train RMSE: 1.0830114801733983 train MAE: 0.8586860727082308\n",
      "Iteration: 15  train RMSE: 1.0684540366717408 train MAE: 0.8468379894194734\n",
      "Iteration: 16  train RMSE: 1.054753705180435 train MAE: 0.8358349166599581\n",
      "Iteration: 17  train RMSE: 1.0418630684569319 train MAE: 0.8253353099900416\n",
      "Iteration: 18  train RMSE: 1.0297865168621236 train MAE: 0.8157381618765237\n",
      "Iteration: 19  train RMSE: 1.0184583352491607 train MAE: 0.8063709669566673\n",
      "Iteration: 20  train RMSE: 1.0079165669887404 train MAE: 0.7981713402996028\n",
      "Iteration: 21  train RMSE: 0.9980382588570976 train MAE: 0.7897546876328311\n",
      "Iteration: 22  train RMSE: 0.9889026681629012 train MAE: 0.7829801888120931\n",
      "Iteration: 23  train RMSE: 0.9803323856019578 train MAE: 0.7753607706015978\n",
      "Iteration: 24  train RMSE: 0.9724648006629817 train MAE: 0.7698902279147196\n",
      "Iteration: 25  train RMSE: 0.9650599426615623 train MAE: 0.7628805491506746\n",
      "Iteration: 26  train RMSE: 0.9583080051244885 train MAE: 0.7586183938757202\n",
      "Iteration: 27  train RMSE: 0.9519171134247659 train MAE: 0.7521505519023408\n",
      "Iteration: 28  train RMSE: 0.9461427011492595 train MAE: 0.7490287341420044\n",
      "Iteration: 29  train RMSE: 0.9406424856301007 train MAE: 0.7429519802829343\n",
      "Iteration: 30  train RMSE: 0.9357514720875042 train MAE: 0.7409005543476339\n",
      "Iteration: 31  train RMSE: 0.931030120349374 train MAE: 0.7350451454991948\n",
      "Iteration: 32  train RMSE: 0.9269213957014147 train MAE: 0.7339868696094708\n",
      "Iteration: 33  train RMSE: 0.9228817101316539 train MAE: 0.7282952085511676\n",
      "Iteration: 34  train RMSE: 0.9194493303215959 train MAE: 0.7281365610130597\n",
      "Iteration: 35  train RMSE: 0.9159788592761491 train MAE: 0.7225239320277419\n",
      "Iteration: 36  train RMSE: 0.9131481409177589 train MAE: 0.7232307270664144\n",
      "Iteration: 37  train RMSE: 0.9101574222198031 train MAE: 0.7176471600100675\n",
      "Iteration: 38  train RMSE: 0.9078303301571997 train MAE: 0.7190772586456893\n",
      "Iteration: 39  train RMSE: 0.9052403643569097 train MAE: 0.7135296624154124\n",
      "Iteration: 40  train RMSE: 0.9033396626801433 train MAE: 0.7155597389688949\n",
      "Iteration: 41  train RMSE: 0.9010780330701146 train MAE: 0.7100653863282632\n",
      "Iteration: 42  train RMSE: 0.8995059253253557 train MAE: 0.7125427657665453\n",
      "Iteration: 43  train RMSE: 0.8974960331647409 train MAE: 0.7070591058905169\n",
      "Iteration: 44  train RMSE: 0.8962082512222227 train MAE: 0.7099783067214023\n",
      "Iteration: 45  train RMSE: 0.8943902170580353 train MAE: 0.7045049536079934\n",
      "Iteration: 46  train RMSE: 0.8933246424186299 train MAE: 0.7077287710337685\n",
      "Iteration: 47  train RMSE: 0.8916568262873632 train MAE: 0.7022743089290849\n",
      "Iteration: 48  train RMSE: 0.8907615148740994 train MAE: 0.705692821683117\n",
      "Iteration: 49  train RMSE: 0.8892155240477229 train MAE: 0.7002694853705228\n",
      "Iteration: 50  train RMSE: 0.8884421992516047 train MAE: 0.7038415670506432\n",
      "Iteration: 51  train RMSE: 0.8869989516686666 train MAE: 0.6984475928654846\n",
      "Iteration: 52  train RMSE: 0.8863214213845421 train MAE: 0.7021265365966733\n",
      "Iteration: 53  train RMSE: 0.8849508883909065 train MAE: 0.6967703870149415\n",
      "Iteration: 54  train RMSE: 0.8843347871490708 train MAE: 0.7005081768591991\n",
      "Iteration: 55  train RMSE: 0.8830252482853467 train MAE: 0.6951855195927742\n",
      "Iteration: 56  train RMSE: 0.8824610121181172 train MAE: 0.698980687524448\n",
      "Iteration: 57  train RMSE: 0.8811990651943473 train MAE: 0.6936817876474901\n",
      "Iteration: 58  train RMSE: 0.8806672089672423 train MAE: 0.6975111162298063\n",
      "Iteration: 59  train RMSE: 0.8794390521200938 train MAE: 0.6922428994390601\n",
      "Iteration: 60  train RMSE: 0.878929639723974 train MAE: 0.6960938675167613\n",
      "Iteration: 61  train RMSE: 0.8777269227564741 train MAE: 0.6908520983722052\n",
      "Iteration: 62  train RMSE: 0.8772276703100712 train MAE: 0.6947059475307162\n",
      "Iteration: 63  train RMSE: 0.8760389022690361 train MAE: 0.6894793437221315\n",
      "Iteration: 64  train RMSE: 0.8755424187793344 train MAE: 0.6933193639678403\n",
      "Iteration: 65  train RMSE: 0.8743522253529993 train MAE: 0.6881173402604996\n",
      "Iteration: 66  train RMSE: 0.8738370461360381 train MAE: 0.6919045981795582\n",
      "Iteration: 67  train RMSE: 0.8726448039107754 train MAE: 0.6867395269418118\n",
      "Iteration: 68  train RMSE: 0.8721083297625962 train MAE: 0.6904546500698632\n",
      "Iteration: 69  train RMSE: 0.8709033351940259 train MAE: 0.6853358988404217\n",
      "Iteration: 70  train RMSE: 0.8703360748646133 train MAE: 0.6889680911665109\n",
      "Iteration: 71  train RMSE: 0.869121131500265 train MAE: 0.6838948279030986\n",
      "Iteration: 72  train RMSE: 0.8685397933039692 train MAE: 0.6874811504202019\n",
      "Iteration: 73  train RMSE: 0.8673189220455307 train MAE: 0.682418772091162\n",
      "Iteration: 74  train RMSE: 0.8667098817972 train MAE: 0.6859524602603567\n",
      "Iteration: 75  train RMSE: 0.8654649868112034 train MAE: 0.6809302343725163\n",
      "Final train RMSE for fold: 4 : 0.8654649868112034\n",
      "Final train MAE for fold: 4 : 0.6809302343725163\n",
      "Final test RMSE for fold: 4 : 0.931505740726137\n",
      "Final test MAE for fold: 4 : 0.7325589501361267\n",
      "Iteration: 1  train RMSE: 1.3462304073158244 train MAE: 1.0767436610266703\n",
      "Iteration: 2  train RMSE: 1.3211975942053402 train MAE: 1.0557840762833972\n",
      "Iteration: 3  train RMSE: 1.2967399194965017 train MAE: 1.035341500878585\n",
      "Iteration: 4  train RMSE: 1.272937147587675 train MAE: 1.0154679150678914\n",
      "Iteration: 5  train RMSE: 1.2498737792215642 train MAE: 0.9962564409293062\n",
      "Iteration: 6  train RMSE: 1.2276066849542364 train MAE: 0.9777623087206783\n",
      "Iteration: 7  train RMSE: 1.2061480788882009 train MAE: 0.9599738262324018\n",
      "Iteration: 8  train RMSE: 1.185566042328085 train MAE: 0.9429887164412938\n",
      "Iteration: 9  train RMSE: 1.1658653432717712 train MAE: 0.9267491481032823\n",
      "Iteration: 10  train RMSE: 1.1470766326697015 train MAE: 0.9112797131118349\n",
      "Iteration: 11  train RMSE: 1.1292012960691011 train MAE: 0.8966210107874298\n",
      "Iteration: 12  train RMSE: 1.112257740989109 train MAE: 0.8827442749042969\n",
      "Iteration: 13  train RMSE: 1.0962174983516766 train MAE: 0.8696017924425283\n",
      "Iteration: 14  train RMSE: 1.0810752778079296 train MAE: 0.8572290664997508\n",
      "Iteration: 15  train RMSE: 1.0667746762780217 train MAE: 0.8455390949965398\n",
      "Iteration: 16  train RMSE: 1.0533368633937212 train MAE: 0.834614112962169\n",
      "Iteration: 17  train RMSE: 1.0407091843771272 train MAE: 0.8243499725731698\n",
      "Iteration: 18  train RMSE: 1.0288821074563226 train MAE: 0.8147532024384501\n",
      "Iteration: 19  train RMSE: 1.0177915695269484 train MAE: 0.8057507626923731\n",
      "Iteration: 20  train RMSE: 1.0074354226149858 train MAE: 0.7974102027292019\n",
      "Iteration: 21  train RMSE: 0.9977480816576203 train MAE: 0.7895440441302273\n",
      "Iteration: 22  train RMSE: 0.9887197810257004 train MAE: 0.7823238217741282\n",
      "Iteration: 23  train RMSE: 0.9802946328438775 train MAE: 0.7754435402887374\n",
      "Iteration: 24  train RMSE: 0.9724583942038315 train MAE: 0.7692969983814673\n",
      "Iteration: 25  train RMSE: 0.9651373943714068 train MAE: 0.7631262195176819\n",
      "Iteration: 26  train RMSE: 0.9583828294620502 train MAE: 0.7581706778572359\n",
      "Iteration: 27  train RMSE: 0.9520530712118962 train MAE: 0.7523633306428842\n",
      "Iteration: 28  train RMSE: 0.9462865895791922 train MAE: 0.7487983506825465\n",
      "Iteration: 29  train RMSE: 0.9408446707189129 train MAE: 0.7430300016070236\n",
      "Iteration: 30  train RMSE: 0.9360030310808268 train MAE: 0.7409776893903421\n",
      "Iteration: 31  train RMSE: 0.9313576682737572 train MAE: 0.7351459472808036\n",
      "Iteration: 32  train RMSE: 0.9273064485492715 train MAE: 0.7342862855575949\n",
      "Iteration: 33  train RMSE: 0.9233234763770904 train MAE: 0.7285762733288479\n",
      "Iteration: 34  train RMSE: 0.9199311325695182 train MAE: 0.7286007735897174\n",
      "Iteration: 35  train RMSE: 0.9164989317156916 train MAE: 0.7229711926519173\n",
      "Iteration: 36  train RMSE: 0.9136846656157337 train MAE: 0.7238496532075253\n",
      "Iteration: 37  train RMSE: 0.9107065093097327 train MAE: 0.718202205546812\n",
      "Iteration: 38  train RMSE: 0.9083704511259317 train MAE: 0.7197950494889602\n",
      "Iteration: 39  train RMSE: 0.90576586092323 train MAE: 0.714151464953702\n",
      "Iteration: 40  train RMSE: 0.9038309762527161 train MAE: 0.716291267229524\n",
      "Iteration: 41  train RMSE: 0.9015338302182017 train MAE: 0.7106718624486005\n",
      "Iteration: 42  train RMSE: 0.8999188085941313 train MAE: 0.7132220067871561\n",
      "Iteration: 43  train RMSE: 0.8978768303754399 train MAE: 0.7076462242453344\n",
      "Iteration: 44  train RMSE: 0.8965270049626481 train MAE: 0.7105355002692504\n",
      "Iteration: 45  train RMSE: 0.8946901747272957 train MAE: 0.705017581559619\n",
      "Iteration: 46  train RMSE: 0.8935541418870833 train MAE: 0.7081636281703921\n",
      "Iteration: 47  train RMSE: 0.8918909764901993 train MAE: 0.7026889036255156\n",
      "Iteration: 48  train RMSE: 0.8909322181566025 train MAE: 0.706053383180922\n",
      "Iteration: 49  train RMSE: 0.8894087783044275 train MAE: 0.7006203966761135\n",
      "Iteration: 50  train RMSE: 0.8885813743230749 train MAE: 0.7041292385897591\n",
      "Iteration: 51  train RMSE: 0.8871766859223851 train MAE: 0.6987458610288977\n",
      "Iteration: 52  train RMSE: 0.8864687875929439 train MAE: 0.7023951969828941\n",
      "Iteration: 53  train RMSE: 0.8851633156250318 train MAE: 0.6970458129429782\n",
      "Iteration: 54  train RMSE: 0.8845414428636476 train MAE: 0.7007883825439282\n",
      "Iteration: 55  train RMSE: 0.8833063427654434 train MAE: 0.6954760260370965\n",
      "Iteration: 56  train RMSE: 0.8827627650441465 train MAE: 0.6993175566242439\n",
      "Iteration: 57  train RMSE: 0.8815935418813676 train MAE: 0.6940334414941814\n",
      "Iteration: 58  train RMSE: 0.8811250073095521 train MAE: 0.6980068355493547\n",
      "Iteration: 59  train RMSE: 0.8799962254448047 train MAE: 0.6927150647990756\n",
      "Iteration: 60  train RMSE: 0.8795592230864706 train MAE: 0.6967296010337901\n",
      "Iteration: 61  train RMSE: 0.8784598767563299 train MAE: 0.6914676375433635\n",
      "Iteration: 62  train RMSE: 0.8780473950843549 train MAE: 0.6955023762693557\n",
      "Iteration: 63  train RMSE: 0.8769721002753483 train MAE: 0.6902596469069087\n",
      "Iteration: 64  train RMSE: 0.8765755353675327 train MAE: 0.6943056802142823\n",
      "Iteration: 65  train RMSE: 0.8755138955121009 train MAE: 0.6890752421693723\n",
      "Iteration: 66  train RMSE: 0.8751255206354832 train MAE: 0.6931187141105185\n",
      "Iteration: 67  train RMSE: 0.8740746282606247 train MAE: 0.6879039167642015\n",
      "Iteration: 68  train RMSE: 0.873683725651588 train MAE: 0.6919352329282306\n",
      "Iteration: 69  train RMSE: 0.8726268301049623 train MAE: 0.6867372204491328\n",
      "Iteration: 70  train RMSE: 0.8722215904503486 train MAE: 0.6907246224169447\n",
      "Iteration: 71  train RMSE: 0.8711659277793281 train MAE: 0.6855490317737332\n",
      "Iteration: 72  train RMSE: 0.8707576228260622 train MAE: 0.6895247394771132\n",
      "Iteration: 73  train RMSE: 0.8696957955139467 train MAE: 0.6843652282942533\n",
      "Iteration: 74  train RMSE: 0.8692724326152779 train MAE: 0.6882914012386486\n",
      "Iteration: 75  train RMSE: 0.8682041186950741 train MAE: 0.6831593522918874\n",
      "Final train RMSE for fold: 5 : 0.8682041186950741\n",
      "Final train MAE for fold: 5 : 0.6831593522918874\n",
      "Final test RMSE for fold: 5 : 0.9327057505952532\n",
      "Final test MAE for fold: 5 : 0.732939569497922\n",
      "Overall Train RMSE: 0.8681080728004673\n",
      "Overall Train MAE: 0.6849282498652222\n",
      "Overall Test RMSE: 0.9325130288229453\n",
      "Overall Test MAE: 0.7351606882096846\n"
     ]
    }
   ],
   "source": [
    "model2=MatrixFactorization(K=15)\n",
    "(plot2_train,plot2_test)=cross_validation(model2, data_df, num_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed7523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  train RMSE: 1.5917782026521192 train MAE: 1.2389028045029917\n",
      "Iteration: 2  train RMSE: 1.5688163391733634 train MAE: 1.219992904595072\n",
      "Iteration: 3  train RMSE: 1.5449106977266873 train MAE: 1.2005362687852856\n",
      "Iteration: 4  train RMSE: 1.5201899683465296 train MAE: 1.1805857948824034\n",
      "Iteration: 5  train RMSE: 1.4948153668572723 train MAE: 1.1603015554848\n",
      "Iteration: 6  train RMSE: 1.4689630230381086 train MAE: 1.1398556374970341\n",
      "Iteration: 7  train RMSE: 1.4427785322816773 train MAE: 1.1193212860696626\n",
      "Iteration: 8  train RMSE: 1.4163766932856567 train MAE: 1.0987898305554382\n",
      "Iteration: 9  train RMSE: 1.389861432769145 train MAE: 1.0783104365241791\n",
      "Iteration: 10  train RMSE: 1.3633715666377673 train MAE: 1.0578811905956766\n",
      "Iteration: 11  train RMSE: 1.3371074312238087 train MAE: 1.0377568207379912\n",
      "Iteration: 12  train RMSE: 1.311200576754012 train MAE: 1.0180264261387744\n",
      "Iteration: 13  train RMSE: 1.285822325139142 train MAE: 0.998812307327654\n",
      "Iteration: 14  train RMSE: 1.2610787800747036 train MAE: 0.9801515468825024\n",
      "Iteration: 15  train RMSE: 1.2370257721362765 train MAE: 0.9620531432906503\n",
      "Iteration: 16  train RMSE: 1.2137491795877298 train MAE: 0.9446113242316369\n",
      "Iteration: 17  train RMSE: 1.1913217732577437 train MAE: 0.9278184433746524\n",
      "Iteration: 18  train RMSE: 1.1698391758966342 train MAE: 0.9117977168203004\n",
      "Iteration: 19  train RMSE: 1.1493621810757173 train MAE: 0.8965843285091661\n",
      "Iteration: 20  train RMSE: 1.1299370253326517 train MAE: 0.8822088122627527\n",
      "Iteration: 21  train RMSE: 1.111576869927669 train MAE: 0.8686031014828477\n",
      "Iteration: 22  train RMSE: 1.094279850769077 train MAE: 0.8558201052223073\n",
      "Iteration: 23  train RMSE: 1.0780142009895468 train MAE: 0.8437691131676194\n",
      "Iteration: 24  train RMSE: 1.0627684923281706 train MAE: 0.8325258774150602\n",
      "Iteration: 25  train RMSE: 1.0484998055767334 train MAE: 0.8219464035095728\n",
      "Iteration: 26  train RMSE: 1.0351773838932734 train MAE: 0.8121094892471697\n",
      "Iteration: 27  train RMSE: 1.022711507969792 train MAE: 0.8027949529181305\n",
      "Iteration: 28  train RMSE: 1.011112692336344 train MAE: 0.7942616874616618\n",
      "Iteration: 29  train RMSE: 1.000273864891382 train MAE: 0.7861040789742231\n",
      "Iteration: 30  train RMSE: 0.9902331053153067 train MAE: 0.7788692431430931\n",
      "Iteration: 31  train RMSE: 0.9808322271756716 train MAE: 0.7714339663688776\n",
      "Iteration: 32  train RMSE: 0.9722332938158449 train MAE: 0.7657285757706372\n",
      "Iteration: 33  train RMSE: 0.9641292286676396 train MAE: 0.7586510096515243\n",
      "Iteration: 34  train RMSE: 0.9568268656155079 train MAE: 0.7545817902208086\n",
      "Iteration: 35  train RMSE: 0.9498842177294128 train MAE: 0.7476175325254462\n",
      "Iteration: 36  train RMSE: 0.9437388656654604 train MAE: 0.7450888879733272\n",
      "Iteration: 37  train RMSE: 0.9378091631069926 train MAE: 0.7382470716788431\n",
      "Iteration: 38  train RMSE: 0.9326523495475036 train MAE: 0.7369653608696344\n",
      "Iteration: 39  train RMSE: 0.9275786564320728 train MAE: 0.7302026522035963\n",
      "Iteration: 40  train RMSE: 0.9232816138417751 train MAE: 0.7300613001272298\n",
      "Iteration: 41  train RMSE: 0.9189342903441662 train MAE: 0.7233793096380241\n",
      "Iteration: 42  train RMSE: 0.9153626681036745 train MAE: 0.7242023819803388\n",
      "Iteration: 43  train RMSE: 0.9116218912051386 train MAE: 0.7175881746518675\n",
      "Iteration: 44  train RMSE: 0.9086745575518804 train MAE: 0.719243563539466\n",
      "Iteration: 45  train RMSE: 0.9054329941576168 train MAE: 0.7126944740118849\n",
      "Iteration: 46  train RMSE: 0.9030047797919125 train MAE: 0.7150028783446231\n",
      "Iteration: 47  train RMSE: 0.9001851349797155 train MAE: 0.7085196383003278\n",
      "Iteration: 48  train RMSE: 0.8981936292324175 train MAE: 0.7113700920966393\n",
      "Iteration: 49  train RMSE: 0.8957104029799368 train MAE: 0.7049372492258962\n",
      "Iteration: 50  train RMSE: 0.8940732261744722 train MAE: 0.7082256515572675\n",
      "Iteration: 51  train RMSE: 0.8918581216674045 train MAE: 0.7018515328261963\n",
      "Iteration: 52  train RMSE: 0.8905010959497223 train MAE: 0.705457249288601\n",
      "Iteration: 53  train RMSE: 0.8885073316713741 train MAE: 0.6991349648454142\n",
      "Iteration: 54  train RMSE: 0.8873736934732989 train MAE: 0.7030103515885867\n",
      "Iteration: 55  train RMSE: 0.8855501539624538 train MAE: 0.6967464879811894\n",
      "Iteration: 56  train RMSE: 0.884580150125503 train MAE: 0.7008010013866803\n",
      "Iteration: 57  train RMSE: 0.8828874400254765 train MAE: 0.6945866601551899\n",
      "Iteration: 58  train RMSE: 0.882047073580362 train MAE: 0.6987897490112697\n",
      "Iteration: 59  train RMSE: 0.880461718880994 train MAE: 0.692589168869751\n",
      "Iteration: 60  train RMSE: 0.879710802898981 train MAE: 0.6968963959649908\n",
      "Iteration: 61  train RMSE: 0.8782012767752914 train MAE: 0.6907200424920249\n",
      "Iteration: 62  train RMSE: 0.8775243517796091 train MAE: 0.695124156796046\n",
      "Iteration: 63  train RMSE: 0.8760738126963834 train MAE: 0.6889719795837926\n",
      "Iteration: 64  train RMSE: 0.8754424896918426 train MAE: 0.6934153948631392\n",
      "Iteration: 65  train RMSE: 0.8740390873546776 train MAE: 0.6872945098804732\n",
      "Iteration: 66  train RMSE: 0.8734539058682185 train MAE: 0.6917977019180075\n",
      "Iteration: 67  train RMSE: 0.8720906768103505 train MAE: 0.6856882877211976\n",
      "Iteration: 68  train RMSE: 0.8715379825491456 train MAE: 0.6902303711007454\n",
      "Iteration: 69  train RMSE: 0.8701967462737519 train MAE: 0.6841381842244613\n",
      "Iteration: 70  train RMSE: 0.8696582277641337 train MAE: 0.6886882599744057\n",
      "Iteration: 71  train RMSE: 0.8683375066898836 train MAE: 0.6826177582834432\n",
      "Iteration: 72  train RMSE: 0.8678138261818228 train MAE: 0.6871792859139216\n",
      "Iteration: 73  train RMSE: 0.8665011053969456 train MAE: 0.6811151522087029\n",
      "Iteration: 74  train RMSE: 0.8659865484629072 train MAE: 0.6856738057625739\n",
      "Iteration: 75  train RMSE: 0.864685665787313 train MAE: 0.6796286993327691\n",
      "Final train RMSE for fold: 1 : 0.864685665787313\n",
      "Final train MAE for fold: 1 : 0.6796286993327691\n",
      "Final test RMSE for fold: 1 : 0.9388042722896444\n",
      "Final test MAE for fold: 1 : 0.7379887701430207\n",
      "Iteration: 1  train RMSE: 1.5880226932507409 train MAE: 1.2332764851352398\n",
      "Iteration: 2  train RMSE: 1.564643209043604 train MAE: 1.2142098288420482\n",
      "Iteration: 3  train RMSE: 1.5404348525563036 train MAE: 1.194712263376423\n",
      "Iteration: 4  train RMSE: 1.51548906258856 train MAE: 1.1748067201632675\n",
      "Iteration: 5  train RMSE: 1.4899450178484788 train MAE: 1.1545667587850124\n",
      "Iteration: 6  train RMSE: 1.4639518174420458 train MAE: 1.1341447653489036\n",
      "Iteration: 7  train RMSE: 1.4376522018647542 train MAE: 1.113640168524121\n",
      "Iteration: 8  train RMSE: 1.411249541331893 train MAE: 1.0932299339685365\n",
      "Iteration: 9  train RMSE: 1.3848876732394324 train MAE: 1.0729591043826718\n",
      "Iteration: 10  train RMSE: 1.3587508831189694 train MAE: 1.053059561384762\n",
      "Iteration: 11  train RMSE: 1.3329518773466935 train MAE: 1.033569741952635\n",
      "Iteration: 12  train RMSE: 1.3076522432622673 train MAE: 1.0145675367008544\n",
      "Iteration: 13  train RMSE: 1.2829727362804737 train MAE: 0.9961240735788386\n",
      "Iteration: 14  train RMSE: 1.2589577766801203 train MAE: 0.978183079639129\n",
      "Iteration: 15  train RMSE: 1.2356617520223996 train MAE: 0.9607636565654329\n",
      "Iteration: 16  train RMSE: 1.2131910395331382 train MAE: 0.9439717529298549\n",
      "Iteration: 17  train RMSE: 1.1916203156330676 train MAE: 0.9278715000174436\n",
      "Iteration: 18  train RMSE: 1.1709512055261915 train MAE: 0.9124607848298785\n",
      "Iteration: 19  train RMSE: 1.1511825523698764 train MAE: 0.8977248631646204\n",
      "Iteration: 20  train RMSE: 1.1323339259088865 train MAE: 0.8836902737007489\n",
      "Iteration: 21  train RMSE: 1.114402211428096 train MAE: 0.8703429975443272\n",
      "Iteration: 22  train RMSE: 1.097427763141244 train MAE: 0.8577819890043963\n",
      "Iteration: 23  train RMSE: 1.0813966804088484 train MAE: 0.8459113003951609\n",
      "Iteration: 24  train RMSE: 1.066286251258471 train MAE: 0.8347135909422714\n",
      "Iteration: 25  train RMSE: 1.052092417615652 train MAE: 0.8241907482901139\n",
      "Iteration: 26  train RMSE: 1.038797903978884 train MAE: 0.8143490112105239\n",
      "Iteration: 27  train RMSE: 1.0263529632196917 train MAE: 0.805142418636168\n",
      "Iteration: 28  train RMSE: 1.0147436752562942 train MAE: 0.7965175702332119\n",
      "Iteration: 29  train RMSE: 1.0039213294559641 train MAE: 0.7885428412855686\n",
      "Iteration: 30  train RMSE: 0.9938184780394438 train MAE: 0.7809932029429425\n",
      "Iteration: 31  train RMSE: 0.9844135932344251 train MAE: 0.7741075712621011\n",
      "Iteration: 32  train RMSE: 0.9756383054444162 train MAE: 0.7674524916536344\n",
      "Iteration: 33  train RMSE: 0.9674801808052883 train MAE: 0.7615456122763089\n",
      "Iteration: 34  train RMSE: 0.9598544618313853 train MAE: 0.7555040766349849\n",
      "Iteration: 35  train RMSE: 0.9528193651335813 train MAE: 0.7507824756094229\n",
      "Iteration: 36  train RMSE: 0.9462014093198996 train MAE: 0.7447376205139878\n",
      "Iteration: 37  train RMSE: 0.9402641330963796 train MAE: 0.7417798921653329\n",
      "Iteration: 38  train RMSE: 0.934595578064138 train MAE: 0.7352410255019405\n",
      "Iteration: 39  train RMSE: 0.9297152011109666 train MAE: 0.7343185641475697\n",
      "Iteration: 40  train RMSE: 0.9248950045779178 train MAE: 0.7273859109396614\n",
      "Iteration: 41  train RMSE: 0.9208994413019619 train MAE: 0.7279449446152243\n",
      "Iteration: 42  train RMSE: 0.9167553386177348 train MAE: 0.7210158340621459\n",
      "Iteration: 43  train RMSE: 0.9134340966769594 train MAE: 0.7224339371063627\n",
      "Iteration: 44  train RMSE: 0.9098468059811367 train MAE: 0.7156342054988016\n",
      "Iteration: 45  train RMSE: 0.9071064247313205 train MAE: 0.7177634399284953\n",
      "Iteration: 46  train RMSE: 0.9039915051416029 train MAE: 0.7110677804706712\n",
      "Iteration: 47  train RMSE: 0.901735490257204 train MAE: 0.7137817475064564\n",
      "Iteration: 48  train RMSE: 0.8990008246388607 train MAE: 0.7071798355633039\n",
      "Iteration: 49  train RMSE: 0.8971472919825934 train MAE: 0.7103552556763777\n",
      "Iteration: 50  train RMSE: 0.8947309735752386 train MAE: 0.7038171156217983\n",
      "Iteration: 51  train RMSE: 0.8932111788100222 train MAE: 0.7073811542490346\n",
      "Iteration: 52  train RMSE: 0.8910372839720311 train MAE: 0.7009126509969323\n",
      "Iteration: 53  train RMSE: 0.8897602514810276 train MAE: 0.7047312556589086\n",
      "Iteration: 54  train RMSE: 0.8877802891393893 train MAE: 0.6983360919391798\n",
      "Iteration: 55  train RMSE: 0.8867072049034175 train MAE: 0.7023892223471693\n",
      "Iteration: 56  train RMSE: 0.8848883583919692 train MAE: 0.696027809714969\n",
      "Iteration: 57  train RMSE: 0.8839537865594134 train MAE: 0.7002386476877545\n",
      "Iteration: 58  train RMSE: 0.8822527647216072 train MAE: 0.6939143070079395\n",
      "Iteration: 59  train RMSE: 0.8814291471932892 train MAE: 0.6982549921787794\n",
      "Iteration: 60  train RMSE: 0.8798265419718753 train MAE: 0.6919641165565255\n",
      "Iteration: 61  train RMSE: 0.879088494202376 train MAE: 0.6963914152952058\n",
      "Iteration: 62  train RMSE: 0.8775605727156102 train MAE: 0.6901257030529644\n",
      "Iteration: 63  train RMSE: 0.8768905040453611 train MAE: 0.6946261502109485\n",
      "Iteration: 64  train RMSE: 0.8754313583961352 train MAE: 0.688385663191455\n",
      "Iteration: 65  train RMSE: 0.8748181000018311 train MAE: 0.6929475960419745\n",
      "Iteration: 66  train RMSE: 0.8734102684203547 train MAE: 0.6867280343108596\n",
      "Iteration: 67  train RMSE: 0.8728426885776815 train MAE: 0.6913407062648202\n",
      "Iteration: 68  train RMSE: 0.8714841325352973 train MAE: 0.6851449344588998\n",
      "Iteration: 69  train RMSE: 0.870955448919686 train MAE: 0.6897991635712831\n",
      "Iteration: 70  train RMSE: 0.8696319459120129 train MAE: 0.6836256603869539\n",
      "Iteration: 71  train RMSE: 0.8691277589495012 train MAE: 0.6883058783038118\n",
      "Iteration: 72  train RMSE: 0.8678278373895655 train MAE: 0.6821546100487775\n",
      "Iteration: 73  train RMSE: 0.8673509267070981 train MAE: 0.6868541727405231\n",
      "Iteration: 74  train RMSE: 0.8660765821422476 train MAE: 0.6807148269940968\n",
      "Iteration: 75  train RMSE: 0.8656082981728168 train MAE: 0.6854251937331375\n",
      "Final train RMSE for fold: 2 : 0.8656082981728168\n",
      "Final train MAE for fold: 2 : 0.6854251937331375\n",
      "Final test RMSE for fold: 2 : 0.9403045685656987\n",
      "Final test MAE for fold: 2 : 0.7440626029318602\n",
      "Iteration: 1  train RMSE: 1.5877487332309836 train MAE: 1.233597848723096\n",
      "Iteration: 2  train RMSE: 1.564624509163623 train MAE: 1.2146333352650747\n",
      "Iteration: 3  train RMSE: 1.5405859237475972 train MAE: 1.1951709619893285\n",
      "Iteration: 4  train RMSE: 1.5157785333591205 train MAE: 1.1752641005050832\n",
      "Iteration: 5  train RMSE: 1.4902913261205855 train MAE: 1.1549918137353188\n",
      "Iteration: 6  train RMSE: 1.4643443978526245 train MAE: 1.1345679096181185\n",
      "Iteration: 7  train RMSE: 1.4380888340857942 train MAE: 1.11408477391621\n",
      "Iteration: 8  train RMSE: 1.4116747624436217 train MAE: 1.0936580184932432\n",
      "Iteration: 9  train RMSE: 1.3851947187142681 train MAE: 1.0733405386616794\n",
      "Iteration: 10  train RMSE: 1.3587960498917273 train MAE: 1.0532195390277697\n",
      "Iteration: 11  train RMSE: 1.3326548886639937 train MAE: 1.03342386972312\n",
      "Iteration: 12  train RMSE: 1.3069337938608763 train MAE: 1.0140096601566597\n",
      "Iteration: 13  train RMSE: 1.281785652656362 train MAE: 0.9950632346420392\n",
      "Iteration: 14  train RMSE: 1.257319712423772 train MAE: 0.976734725849247\n",
      "Iteration: 15  train RMSE: 1.2336216284921206 train MAE: 0.9590374766022856\n",
      "Iteration: 16  train RMSE: 1.2107569489350951 train MAE: 0.9419867684388769\n",
      "Iteration: 17  train RMSE: 1.1888142672230557 train MAE: 0.9256314822077533\n",
      "Iteration: 18  train RMSE: 1.1677827894845518 train MAE: 0.9099249797940246\n",
      "Iteration: 19  train RMSE: 1.1477416670018918 train MAE: 0.894999996860031\n",
      "Iteration: 20  train RMSE: 1.128710818792513 train MAE: 0.8808374550541108\n",
      "Iteration: 21  train RMSE: 1.1107009964295664 train MAE: 0.8674762918331265\n",
      "Iteration: 22  train RMSE: 1.0937020298907327 train MAE: 0.8548034924699163\n",
      "Iteration: 23  train RMSE: 1.0777343843403684 train MAE: 0.8429628932567773\n",
      "Iteration: 24  train RMSE: 1.0627251732590473 train MAE: 0.8317255982554029\n",
      "Iteration: 25  train RMSE: 1.048694076499771 train MAE: 0.8213181441247662\n",
      "Iteration: 26  train RMSE: 1.035570921211783 train MAE: 0.8115162820850821\n",
      "Iteration: 27  train RMSE: 1.0233408066120178 train MAE: 0.8024795216737398\n",
      "Iteration: 28  train RMSE: 1.0119201473753343 train MAE: 0.7938747215498796\n",
      "Iteration: 29  train RMSE: 1.001321160652706 train MAE: 0.7861178755155145\n",
      "Iteration: 30  train RMSE: 0.9914267760895324 train MAE: 0.778538296239094\n",
      "Iteration: 31  train RMSE: 0.9822944119134613 train MAE: 0.7721203167676209\n",
      "Iteration: 32  train RMSE: 0.9737322363013979 train MAE: 0.7651277262596715\n",
      "Iteration: 33  train RMSE: 0.9659147619352942 train MAE: 0.7602022970361055\n",
      "Iteration: 34  train RMSE: 0.9585231296780082 train MAE: 0.7533767666666863\n",
      "Iteration: 35  train RMSE: 0.9518747554521068 train MAE: 0.7500690511744686\n",
      "Iteration: 36  train RMSE: 0.9454969901459147 train MAE: 0.7432428584562727\n",
      "Iteration: 37  train RMSE: 0.93987551685216 train MAE: 0.7413918530028343\n",
      "Iteration: 38  train RMSE: 0.9343931689965282 train MAE: 0.7345944141427762\n",
      "Iteration: 39  train RMSE: 0.9296882559098234 train MAE: 0.734007758923607\n",
      "Iteration: 40  train RMSE: 0.9249772620487435 train MAE: 0.7272999355326412\n",
      "Iteration: 41  train RMSE: 0.9210369717947654 train MAE: 0.7277184425462855\n",
      "Iteration: 42  train RMSE: 0.9169741309866564 train MAE: 0.7210704654862519\n",
      "Iteration: 43  train RMSE: 0.9137008647393269 train MAE: 0.7223812488138396\n",
      "Iteration: 44  train RMSE: 0.9101746199582982 train MAE: 0.7157629309890445\n",
      "Iteration: 45  train RMSE: 0.9074624678277472 train MAE: 0.717787810782343\n",
      "Iteration: 46  train RMSE: 0.9043845827867143 train MAE: 0.711214375878087\n",
      "Iteration: 47  train RMSE: 0.9021581401219557 train MAE: 0.7138648758933446\n",
      "Iteration: 48  train RMSE: 0.8994428993413712 train MAE: 0.7073392689234134\n",
      "Iteration: 49  train RMSE: 0.8975847129523613 train MAE: 0.7104250369121381\n",
      "Iteration: 50  train RMSE: 0.8951547777119934 train MAE: 0.7039681453221133\n",
      "Iteration: 51  train RMSE: 0.8935981131267583 train MAE: 0.707417875845777\n",
      "Iteration: 52  train RMSE: 0.8914016800331818 train MAE: 0.7010188590254046\n",
      "Iteration: 53  train RMSE: 0.8900931977704133 train MAE: 0.7047640365985806\n",
      "Iteration: 54  train RMSE: 0.8880777036815457 train MAE: 0.6984073208643895\n",
      "Iteration: 55  train RMSE: 0.8869517219218672 train MAE: 0.7023496182472083\n",
      "Iteration: 56  train RMSE: 0.8850838202372835 train MAE: 0.696032892298465\n",
      "Iteration: 57  train RMSE: 0.8841113023269319 train MAE: 0.7001383991334571\n",
      "Iteration: 58  train RMSE: 0.8823639706159973 train MAE: 0.6938590762723015\n",
      "Iteration: 59  train RMSE: 0.881512539379476 train MAE: 0.6980874609340293\n",
      "Iteration: 60  train RMSE: 0.8798642559052393 train MAE: 0.6918358535125755\n",
      "Iteration: 61  train RMSE: 0.8790950330191146 train MAE: 0.6961564335892748\n",
      "Iteration: 62  train RMSE: 0.8775204480279263 train MAE: 0.689941391598853\n",
      "Iteration: 63  train RMSE: 0.8768156527800071 train MAE: 0.694320376474036\n",
      "Iteration: 64  train RMSE: 0.8753046934893532 train MAE: 0.688136202805846\n",
      "Iteration: 65  train RMSE: 0.8746487321650261 train MAE: 0.6925712164230513\n",
      "Iteration: 66  train RMSE: 0.873181947966824 train MAE: 0.6864122830803813\n",
      "Iteration: 67  train RMSE: 0.8725618555517043 train MAE: 0.6908802106733554\n",
      "Iteration: 68  train RMSE: 0.8711331849034426 train MAE: 0.6847418440400947\n",
      "Iteration: 69  train RMSE: 0.8705373381939846 train MAE: 0.6892386277005111\n",
      "Iteration: 70  train RMSE: 0.8691321892667053 train MAE: 0.6831291755506868\n",
      "Iteration: 71  train RMSE: 0.8685463575599958 train MAE: 0.6876164086658727\n",
      "Iteration: 72  train RMSE: 0.867153028020834 train MAE: 0.6815355914000606\n",
      "Iteration: 73  train RMSE: 0.8665694760573462 train MAE: 0.686001564658466\n",
      "Iteration: 74  train RMSE: 0.8651840557744952 train MAE: 0.679945793737778\n",
      "Iteration: 75  train RMSE: 0.8645974581280087 train MAE: 0.684382336222646\n",
      "Final train RMSE for fold: 3 : 0.8645974581280087\n",
      "Final train MAE for fold: 3 : 0.684382336222646\n",
      "Final test RMSE for fold: 3 : 0.9383950592799432\n",
      "Final test MAE for fold: 3 : 0.7433372982831634\n",
      "Iteration: 1  train RMSE: 1.5849506519457455 train MAE: 1.2327996077353292\n",
      "Iteration: 2  train RMSE: 1.562023479129462 train MAE: 1.214072613700842\n",
      "Iteration: 3  train RMSE: 1.5382198095237776 train MAE: 1.194775414220781\n",
      "Iteration: 4  train RMSE: 1.5136182111987644 train MAE: 1.174942629955217\n",
      "Iteration: 5  train RMSE: 1.4883944573295869 train MAE: 1.1547508168043423\n",
      "Iteration: 6  train RMSE: 1.4626579649944058 train MAE: 1.1342794530596598\n",
      "Iteration: 7  train RMSE: 1.4366173865653857 train MAE: 1.1137422903297585\n",
      "Iteration: 8  train RMSE: 1.4103670764889304 train MAE: 1.0931786436400002\n",
      "Iteration: 9  train RMSE: 1.3840537536154414 train MAE: 1.072680420586584\n",
      "Iteration: 10  train RMSE: 1.3578162824722413 train MAE: 1.0523520491840928\n",
      "Iteration: 11  train RMSE: 1.331854485102648 train MAE: 1.0323765579318436\n",
      "Iteration: 12  train RMSE: 1.3062608586712818 train MAE: 1.012717400955277\n",
      "Iteration: 13  train RMSE: 1.2812402493445414 train MAE: 0.9936230283313247\n",
      "Iteration: 14  train RMSE: 1.2568544039944232 train MAE: 0.9750749130152947\n",
      "Iteration: 15  train RMSE: 1.2332365452201217 train MAE: 0.9572595204654661\n",
      "Iteration: 16  train RMSE: 1.2104692777252442 train MAE: 0.9401504454472013\n",
      "Iteration: 17  train RMSE: 1.1885936927159588 train MAE: 0.9237617007100106\n",
      "Iteration: 18  train RMSE: 1.167649822725992 train MAE: 0.9080914714678296\n",
      "Iteration: 19  train RMSE: 1.1476814121016714 train MAE: 0.8932051083600496\n",
      "Iteration: 20  train RMSE: 1.1287116647164857 train MAE: 0.8790684214019697\n",
      "Iteration: 21  train RMSE: 1.1107915254331027 train MAE: 0.8658348568401404\n",
      "Iteration: 22  train RMSE: 1.0938769314993648 train MAE: 0.8533148870148183\n",
      "Iteration: 23  train RMSE: 1.077947951348354 train MAE: 0.8415849220275161\n",
      "Iteration: 24  train RMSE: 1.062959900376653 train MAE: 0.830505440932642\n",
      "Iteration: 25  train RMSE: 1.0489016137028344 train MAE: 0.8202133471367249\n",
      "Iteration: 26  train RMSE: 1.035705448590228 train MAE: 0.8104166170256532\n",
      "Iteration: 27  train RMSE: 1.0233830926083216 train MAE: 0.8014922002655175\n",
      "Iteration: 28  train RMSE: 1.0118287645325694 train MAE: 0.7927646901737898\n",
      "Iteration: 29  train RMSE: 1.001129393978718 train MAE: 0.7852832176601675\n",
      "Iteration: 30  train RMSE: 0.9910765077526076 train MAE: 0.7773670832095569\n",
      "Iteration: 31  train RMSE: 0.9818772132313787 train MAE: 0.7714082703596045\n",
      "Iteration: 32  train RMSE: 0.9731879478230507 train MAE: 0.76389291153892\n",
      "Iteration: 33  train RMSE: 0.9653485153143767 train MAE: 0.7595268275734559\n",
      "Iteration: 34  train RMSE: 0.957859315879328 train MAE: 0.7522949211933136\n",
      "Iteration: 35  train RMSE: 0.951188695273392 train MAE: 0.7492914090121159\n",
      "Iteration: 36  train RMSE: 0.9447726376630904 train MAE: 0.7423648150783361\n",
      "Iteration: 37  train RMSE: 0.9391475589053163 train MAE: 0.7406049011822362\n",
      "Iteration: 38  train RMSE: 0.93365582932834 train MAE: 0.7338108742115574\n",
      "Iteration: 39  train RMSE: 0.9289674973177974 train MAE: 0.7332858142401523\n",
      "Iteration: 40  train RMSE: 0.9242947519015193 train MAE: 0.7266074048537142\n",
      "Iteration: 41  train RMSE: 0.9204491065944554 train MAE: 0.7272087071319682\n",
      "Iteration: 42  train RMSE: 0.9164588846160147 train MAE: 0.7205490026900168\n",
      "Iteration: 43  train RMSE: 0.9133160733697019 train MAE: 0.7220927224420116\n",
      "Iteration: 44  train RMSE: 0.9098712061439034 train MAE: 0.7154564519786584\n",
      "Iteration: 45  train RMSE: 0.9072748029871218 train MAE: 0.7176900093982264\n",
      "Iteration: 46  train RMSE: 0.9042663153750443 train MAE: 0.7111082618463069\n",
      "Iteration: 47  train RMSE: 0.9021195503917804 train MAE: 0.7139048616400417\n",
      "Iteration: 48  train RMSE: 0.8994645576807355 train MAE: 0.7073576008807375\n",
      "Iteration: 49  train RMSE: 0.8976889343454647 train MAE: 0.7106147088986827\n",
      "Iteration: 50  train RMSE: 0.8953125749368424 train MAE: 0.7041139088992809\n",
      "Iteration: 51  train RMSE: 0.8938261887681109 train MAE: 0.7077095177962501\n",
      "Iteration: 52  train RMSE: 0.8916773676537137 train MAE: 0.7012622652613744\n",
      "Iteration: 53  train RMSE: 0.8904213421258338 train MAE: 0.7051157607215075\n",
      "Iteration: 54  train RMSE: 0.8884469200687873 train MAE: 0.6987025748460186\n",
      "Iteration: 55  train RMSE: 0.8873699375187502 train MAE: 0.7027405732741412\n",
      "Iteration: 56  train RMSE: 0.8855425619218257 train MAE: 0.6963807520722063\n",
      "Iteration: 57  train RMSE: 0.884620069530663 train MAE: 0.7005854442398656\n",
      "Iteration: 58  train RMSE: 0.8829222078859531 train MAE: 0.6942666068495505\n",
      "Iteration: 59  train RMSE: 0.8821285615396399 train MAE: 0.6986071229317076\n",
      "Iteration: 60  train RMSE: 0.8805413061968065 train MAE: 0.6923268022682931\n",
      "Iteration: 61  train RMSE: 0.879845586698563 train MAE: 0.6967778804025552\n",
      "Iteration: 62  train RMSE: 0.8783430364009651 train MAE: 0.6905343458368474\n",
      "Iteration: 63  train RMSE: 0.8777214518459517 train MAE: 0.6950652316604944\n",
      "Iteration: 64  train RMSE: 0.8762890755935528 train MAE: 0.6888607900994789\n",
      "Iteration: 65  train RMSE: 0.8757337035836826 train MAE: 0.6934655908267705\n",
      "Iteration: 66  train RMSE: 0.8743551852048299 train MAE: 0.6872834990524318\n",
      "Iteration: 67  train RMSE: 0.8738517744144947 train MAE: 0.6919416472372082\n",
      "Iteration: 68  train RMSE: 0.8725204659414321 train MAE: 0.6857891358861145\n",
      "Iteration: 69  train RMSE: 0.8720634608624439 train MAE: 0.6904996785670445\n",
      "Iteration: 70  train RMSE: 0.8707640923203483 train MAE: 0.684362276582043\n",
      "Iteration: 71  train RMSE: 0.8703407138057864 train MAE: 0.6891064898591371\n",
      "Iteration: 72  train RMSE: 0.8690727117440903 train MAE: 0.6829707129717917\n",
      "Iteration: 73  train RMSE: 0.8686792079633145 train MAE: 0.6877470040179069\n",
      "Iteration: 74  train RMSE: 0.8674362334997827 train MAE: 0.681618085101039\n",
      "Iteration: 75  train RMSE: 0.8670628059260547 train MAE: 0.686414778272409\n",
      "Final train RMSE for fold: 4 : 0.8670628059260547\n",
      "Final train MAE for fold: 4 : 0.686414778272409\n",
      "Final test RMSE for fold: 4 : 0.9412698764036859\n",
      "Final test MAE for fold: 4 : 0.745579378517222\n",
      "Iteration: 1  train RMSE: 1.5965172753300791 train MAE: 1.2416387844800305\n",
      "Iteration: 2  train RMSE: 1.5733143427647198 train MAE: 1.222308023225365\n",
      "Iteration: 3  train RMSE: 1.5492028994247455 train MAE: 1.2024897180714684\n",
      "Iteration: 4  train RMSE: 1.5242947753605254 train MAE: 1.1822501208352638\n",
      "Iteration: 5  train RMSE: 1.498763587800585 train MAE: 1.1617821645026913\n",
      "Iteration: 6  train RMSE: 1.472730492893239 train MAE: 1.1411179297433067\n",
      "Iteration: 7  train RMSE: 1.4463651573504797 train MAE: 1.1203891210251309\n",
      "Iteration: 8  train RMSE: 1.419826120257384 train MAE: 1.0997422619695534\n",
      "Iteration: 9  train RMSE: 1.3931966587170361 train MAE: 1.0791528940200315\n",
      "Iteration: 10  train RMSE: 1.3666566117532708 train MAE: 1.0587061928978951\n",
      "Iteration: 11  train RMSE: 1.3403654529562194 train MAE: 1.0385510285819812\n",
      "Iteration: 12  train RMSE: 1.3145018167166385 train MAE: 1.0187775396568353\n",
      "Iteration: 13  train RMSE: 1.2891797112027907 train MAE: 0.9994860237873742\n",
      "Iteration: 14  train RMSE: 1.264472674999769 train MAE: 0.9807173900325028\n",
      "Iteration: 15  train RMSE: 1.2404863312387335 train MAE: 0.9625697784578378\n",
      "Iteration: 16  train RMSE: 1.2173291407576317 train MAE: 0.9451317958560557\n",
      "Iteration: 17  train RMSE: 1.1950895720915475 train MAE: 0.9284676742805381\n",
      "Iteration: 18  train RMSE: 1.1737918419776237 train MAE: 0.9125393173591835\n",
      "Iteration: 19  train RMSE: 1.1535269818164815 train MAE: 0.8974678447283301\n",
      "Iteration: 20  train RMSE: 1.1342903548256933 train MAE: 0.8832029067203981\n",
      "Iteration: 21  train RMSE: 1.116092051778037 train MAE: 0.8697869904682499\n",
      "Iteration: 22  train RMSE: 1.0989301355403835 train MAE: 0.8571642397444578\n",
      "Iteration: 23  train RMSE: 1.082780008825497 train MAE: 0.8453504654883749\n",
      "Iteration: 24  train RMSE: 1.0676145625282194 train MAE: 0.8341963761476343\n",
      "Iteration: 25  train RMSE: 1.0534245755717515 train MAE: 0.82384312955979\n",
      "Iteration: 26  train RMSE: 1.0401397339223488 train MAE: 0.8140212365186804\n",
      "Iteration: 27  train RMSE: 1.0277295646092481 train MAE: 0.8049999848702294\n",
      "Iteration: 28  train RMSE: 1.016121303400884 train MAE: 0.7962867706373106\n",
      "Iteration: 29  train RMSE: 1.005328199936571 train MAE: 0.7886533708257483\n",
      "Iteration: 30  train RMSE: 0.9951902709409443 train MAE: 0.7807050513034389\n",
      "Iteration: 31  train RMSE: 0.9858468100794686 train MAE: 0.7745259925334186\n",
      "Iteration: 32  train RMSE: 0.9770096098884103 train MAE: 0.7669967461393572\n",
      "Iteration: 33  train RMSE: 0.9689636641215088 train MAE: 0.7623454853267218\n",
      "Iteration: 34  train RMSE: 0.9613004950009387 train MAE: 0.75508422070502\n",
      "Iteration: 35  train RMSE: 0.9544381889154022 train MAE: 0.7519247730281889\n",
      "Iteration: 36  train RMSE: 0.9478213277873992 train MAE: 0.7448930523993534\n",
      "Iteration: 37  train RMSE: 0.941995420977381 train MAE: 0.743016452476482\n",
      "Iteration: 38  train RMSE: 0.9362948220564409 train MAE: 0.7361484378640535\n",
      "Iteration: 39  train RMSE: 0.9313958207234724 train MAE: 0.7354286483117397\n",
      "Iteration: 40  train RMSE: 0.92649516768264 train MAE: 0.7286405853467479\n",
      "Iteration: 41  train RMSE: 0.9223806823267742 train MAE: 0.7288691149224495\n",
      "Iteration: 42  train RMSE: 0.918167261150624 train MAE: 0.7222146044417711\n",
      "Iteration: 43  train RMSE: 0.9147745945190837 train MAE: 0.7234067780900649\n",
      "Iteration: 44  train RMSE: 0.9111274356352237 train MAE: 0.7167554086940443\n",
      "Iteration: 45  train RMSE: 0.9083047970403157 train MAE: 0.718656332813006\n",
      "Iteration: 46  train RMSE: 0.9051271692768993 train MAE: 0.7120468335081761\n",
      "Iteration: 47  train RMSE: 0.9027931891677333 train MAE: 0.7145714416668384\n",
      "Iteration: 48  train RMSE: 0.8999953091192054 train MAE: 0.7079981500062614\n",
      "Iteration: 49  train RMSE: 0.8980574212911707 train MAE: 0.7110148650074335\n",
      "Iteration: 50  train RMSE: 0.8955585981787432 train MAE: 0.7044841274531413\n",
      "Iteration: 51  train RMSE: 0.8939339222701577 train MAE: 0.7078587847197669\n",
      "Iteration: 52  train RMSE: 0.8916911282755257 train MAE: 0.7014096427012112\n",
      "Iteration: 53  train RMSE: 0.8903242393828831 train MAE: 0.705092707726742\n",
      "Iteration: 54  train RMSE: 0.8882876269789507 train MAE: 0.6986928144847282\n",
      "Iteration: 55  train RMSE: 0.8871422450558998 train MAE: 0.7026210684266416\n",
      "Iteration: 56  train RMSE: 0.8852765153860306 train MAE: 0.6962725939524445\n",
      "Iteration: 57  train RMSE: 0.884299982790671 train MAE: 0.7003961877684801\n",
      "Iteration: 58  train RMSE: 0.8825797585294576 train MAE: 0.6940996941808132\n",
      "Iteration: 59  train RMSE: 0.8817415906462138 train MAE: 0.6983771535460811\n",
      "Iteration: 60  train RMSE: 0.8801361007352622 train MAE: 0.6921249267099217\n",
      "Iteration: 61  train RMSE: 0.8794015385795951 train MAE: 0.6965188591772589\n",
      "Iteration: 62  train RMSE: 0.8778864257564741 train MAE: 0.6903056336801933\n",
      "Iteration: 63  train RMSE: 0.8772256403325195 train MAE: 0.6947761049013439\n",
      "Iteration: 64  train RMSE: 0.8757841533023406 train MAE: 0.6886045755378902\n",
      "Iteration: 65  train RMSE: 0.8751839431577847 train MAE: 0.6931262853056759\n",
      "Iteration: 66  train RMSE: 0.8738034615853583 train MAE: 0.686978168108246\n",
      "Iteration: 67  train RMSE: 0.8732483882859949 train MAE: 0.6915596632256777\n",
      "Iteration: 68  train RMSE: 0.8719086806877402 train MAE: 0.6854363628702467\n",
      "Iteration: 69  train RMSE: 0.8713852026069749 train MAE: 0.69004336206009\n",
      "Iteration: 70  train RMSE: 0.8700768968704023 train MAE: 0.68393265391096\n",
      "Iteration: 71  train RMSE: 0.8695710892606152 train MAE: 0.6885589028334952\n",
      "Iteration: 72  train RMSE: 0.8682864071629064 train MAE: 0.6824683132862969\n",
      "Iteration: 73  train RMSE: 0.8677970844143597 train MAE: 0.6871029952108956\n",
      "Iteration: 74  train RMSE: 0.8665298867378799 train MAE: 0.6810238432713295\n",
      "Iteration: 75  train RMSE: 0.8660527223445472 train MAE: 0.6856693190919823\n",
      "Final train RMSE for fold: 5 : 0.8660527223445472\n",
      "Final train MAE for fold: 5 : 0.6856693190919823\n",
      "Final test RMSE for fold: 5 : 0.939856954849441\n",
      "Final test MAE for fold: 5 : 0.7443019563014102\n",
      "Overall Train RMSE: 0.8656013900717481\n",
      "Overall Train MAE: 0.6843040653305887\n",
      "Overall Test RMSE: 0.9397261462776825\n",
      "Overall Test MAE: 0.7430540012353354\n"
     ]
    }
   ],
   "source": [
    "model3=MatrixFactorization(K=20)\n",
    "(plot3_train,plot3_test)=cross_validation(model3, data_df, num_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085e2262",
   "metadata": {},
   "source": [
    "For the three Ks that the algorithm was run, a plot with the training RMSE curve for all the folds is ploted, against the iterations. On the same plot, the straight line represents the final overall test RMSE for the specific K. So, for K=10 the results are: \n",
    "\n",
    "Overall Train RMSE: 0.8839608355048773\n",
    "Overall Train MAE: 0.6994310596144893\n",
    "Overall Test RMSE: 0.9253240684611157\n",
    "Overall Test MAE: 0.7316915551664489\n",
    "\n",
    "This means that the predicted ratings do not deviate much from the real ratings and the algortihm can be used to predict the rate that a user would give to a movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40a427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4500x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train=[plot1_train,plot2_train,plot3_train] #list of lists of rmses for each fold\n",
    "plot_test=[plot1_test,plot2_test,plot3_test] #list of three numbers to create strainght lines\n",
    "\n",
    "Ks = [10,15,20]\n",
    "plt.rcParams['font.size'] = '16'\n",
    "figure, axis = plt.subplots(1,3, figsize=(45,10))\n",
    "max_rmse = max(max([max(max(train_fold), plot_test) for train_fold in plot_train]))\n",
    "\n",
    "for i in range(3):\n",
    "  for j in range(len(plot_train[i])):\n",
    "      axis[i].plot(range(1, len(plot_train[i][j]) + 1), plot_train[i][j], linestyle=\"-\", color='blue', label=f'Fold {i + 1} Train')\n",
    "      y=[plot_test[i]]*len(plot_train[i][j])\n",
    "      axis[i].plot(range(1, len(plot_train[i][j]) + 1), y, linestyle=\"-\", color='red', label=f'Fold {i + 1} Test')\n",
    "\n",
    "      axis[i].set_title('Dimension:'+ str(Ks[i]))\n",
    "      axis[i].set_ylabel('RMSE')\n",
    "      axis[i].set_xlabel('iterations')\n",
    "\n",
    "      axis[i].set_ylim(0, max_rmse)\n",
    "\n",
    "train_legend = mlines.Line2D([], [], color='blue', label='Train RMSE')\n",
    "test_legend = mlines.Line2D([], [], color='red', label='Test RMSE')\n",
    "\n",
    "# Adding legends to each subplot\n",
    "axis[0].legend(handles=[train_legend, test_legend])\n",
    "axis[1].legend(handles=[train_legend, test_legend])\n",
    "axis[2].legend(handles=[train_legend, test_legend])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f88185",
   "metadata": {},
   "source": [
    "The lines are very similar and all the models performed rather well. The K with the best performance though, is the one with K=10 as it achieved the smallest overall RMSE and MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76bb25b",
   "metadata": {},
   "source": [
    "## Output for Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700922e",
   "metadata": {},
   "source": [
    "Perform the Matrix Factorization on all the dataset, with the best chosen K=10 and print the U and M matrices for users and movies respectively to be used for the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75acc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1  train RMSE: 1.6723599110932035 train MAE: 1.4053197534357433\n",
      "Iteration: 2  train RMSE: 1.6424333442076908 train MAE: 1.3768959859366652\n",
      "Iteration: 3  train RMSE: 1.6128689302934212 train MAE: 1.348949091650833\n",
      "Iteration: 4  train RMSE: 1.583708783100971 train MAE: 1.3215138803466133\n",
      "Iteration: 5  train RMSE: 1.5549932054781697 train MAE: 1.294630209223266\n",
      "Iteration: 6  train RMSE: 1.5267599777282561 train MAE: 1.268345288749682\n",
      "Iteration: 7  train RMSE: 1.4990453458617221 train MAE: 1.242674486855773\n",
      "Iteration: 8  train RMSE: 1.4718796548901336 train MAE: 1.2176376408447118\n",
      "Iteration: 9  train RMSE: 1.4452912911184237 train MAE: 1.1932584088508473\n",
      "Iteration: 10  train RMSE: 1.4193089893841595 train MAE: 1.1695410544314078\n",
      "Iteration: 11  train RMSE: 1.393959577428204 train MAE: 1.1465210755674582\n",
      "Iteration: 12  train RMSE: 1.3692692765816574 train MAE: 1.124199841761164\n",
      "Iteration: 13  train RMSE: 1.3452615794195448 train MAE: 1.1026005948363748\n",
      "Iteration: 14  train RMSE: 1.3219653210024906 train MAE: 1.0817466919554646\n",
      "Iteration: 15  train RMSE: 1.2993896546404233 train MAE: 1.0616575961144805\n",
      "Iteration: 16  train RMSE: 1.2775467646916379 train MAE: 1.0422991213341504\n",
      "Iteration: 17  train RMSE: 1.2564466137299046 train MAE: 1.0236818430099655\n",
      "Iteration: 18  train RMSE: 1.2360984314651347 train MAE: 1.0057937879629817\n",
      "Iteration: 19  train RMSE: 1.2165140953044613 train MAE: 0.9886351167968427\n",
      "Iteration: 20  train RMSE: 1.1976858110831003 train MAE: 0.9721757146047392\n",
      "Iteration: 21  train RMSE: 1.1796171250832121 train MAE: 0.9564223733015291\n",
      "Iteration: 22  train RMSE: 1.1623034192406898 train MAE: 0.9413663610600154\n",
      "Iteration: 23  train RMSE: 1.145740619076039 train MAE: 0.9269845972318581\n",
      "Iteration: 24  train RMSE: 1.1299195355615181 train MAE: 0.9132806417315144\n",
      "Iteration: 25  train RMSE: 1.1148252773637128 train MAE: 0.9002245162351559\n",
      "Iteration: 26  train RMSE: 1.1004422134061245 train MAE: 0.8877770643521489\n",
      "Iteration: 27  train RMSE: 1.0867642911033988 train MAE: 0.8760021919271757\n",
      "Iteration: 28  train RMSE: 1.073761690229262 train MAE: 0.8647525026995141\n",
      "Iteration: 29  train RMSE: 1.061425365539089 train MAE: 0.8541951433234575\n",
      "Iteration: 30  train RMSE: 1.0497230267537108 train MAE: 0.8439989081903747\n",
      "Iteration: 31  train RMSE: 1.0386732252973352 train MAE: 0.8345888467927476\n",
      "Iteration: 32  train RMSE: 1.0282206791538377 train MAE: 0.8253603220991802\n",
      "Iteration: 33  train RMSE: 1.0183936237232782 train MAE: 0.8170923699576172\n",
      "Iteration: 34  train RMSE: 1.0091139721586555 train MAE: 0.8087831101739864\n",
      "Iteration: 35  train RMSE: 1.0004112698769536 train MAE: 0.8015521432377009\n",
      "Iteration: 36  train RMSE: 0.9922153271713938 train MAE: 0.7940600810840345\n",
      "Iteration: 37  train RMSE: 0.9845716595546568 train MAE: 0.7879109692701781\n",
      "Iteration: 38  train RMSE: 0.9773684204700015 train MAE: 0.7810897830146052\n",
      "Iteration: 39  train RMSE: 0.970686330961118 train MAE: 0.7758725755795447\n",
      "Iteration: 40  train RMSE: 0.9643957142632962 train MAE: 0.7696127430473751\n",
      "Iteration: 41  train RMSE: 0.9585934979252051 train MAE: 0.765335458719384\n",
      "Iteration: 42  train RMSE: 0.9531295650434125 train MAE: 0.7595673975568995\n",
      "Iteration: 43  train RMSE: 0.9481261103038996 train MAE: 0.7561672773548543\n",
      "Iteration: 44  train RMSE: 0.9434091511872665 train MAE: 0.7508083369040699\n",
      "Iteration: 45  train RMSE: 0.9391299133703102 train MAE: 0.7483344443224608\n",
      "Iteration: 46  train RMSE: 0.9350557875531054 train MAE: 0.7432310104857298\n",
      "Iteration: 47  train RMSE: 0.9314017258030691 train MAE: 0.7415944131893846\n",
      "Iteration: 48  train RMSE: 0.927895408625985 train MAE: 0.7367256126696025\n",
      "Iteration: 49  train RMSE: 0.9247944137725329 train MAE: 0.7358390265926517\n",
      "Iteration: 50  train RMSE: 0.9217786076203529 train MAE: 0.7311405539609975\n",
      "Iteration: 51  train RMSE: 0.9191597715144464 train MAE: 0.7309652030147367\n",
      "Iteration: 52  train RMSE: 0.9165789007463945 train MAE: 0.7263845379258399\n",
      "Iteration: 53  train RMSE: 0.914370334793044 train MAE: 0.7268283764789278\n",
      "Iteration: 54  train RMSE: 0.9121415931803845 train MAE: 0.722336099756846\n",
      "Iteration: 55  train RMSE: 0.9102732584519273 train MAE: 0.7232648187412217\n",
      "Iteration: 56  train RMSE: 0.9083549569694536 train MAE: 0.7188526116646983\n",
      "Iteration: 57  train RMSE: 0.9067705899840218 train MAE: 0.7202086809536712\n",
      "Iteration: 58  train RMSE: 0.9051098702771535 train MAE: 0.715855621041283\n",
      "Iteration: 59  train RMSE: 0.9037695761689573 train MAE: 0.7175603146682737\n",
      "Iteration: 60  train RMSE: 0.9023275095917425 train MAE: 0.7132588845671718\n",
      "Iteration: 61  train RMSE: 0.9011936927618748 train MAE: 0.715260825963866\n",
      "Iteration: 62  train RMSE: 0.8999396944350408 train MAE: 0.7110186166862262\n",
      "Iteration: 63  train RMSE: 0.8989752198881755 train MAE: 0.71330755223899\n",
      "Iteration: 64  train RMSE: 0.8978671865958929 train MAE: 0.7090753266847085\n",
      "Iteration: 65  train RMSE: 0.8970320560993202 train MAE: 0.7115525250295378\n",
      "Iteration: 66  train RMSE: 0.8960507821175123 train MAE: 0.7073594014586911\n",
      "Iteration: 67  train RMSE: 0.8953309561389946 train MAE: 0.7100413829048895\n",
      "Iteration: 68  train RMSE: 0.8944481231674809 train MAE: 0.7058649846745774\n",
      "Iteration: 69  train RMSE: 0.8938102208334789 train MAE: 0.7086688415375156\n",
      "Iteration: 70  train RMSE: 0.8929990334343827 train MAE: 0.7045251221161183\n",
      "Iteration: 71  train RMSE: 0.8924390134666867 train MAE: 0.7074352248375931\n",
      "Iteration: 72  train RMSE: 0.8916903801351348 train MAE: 0.7033113248078836\n",
      "Iteration: 73  train RMSE: 0.8911962789966058 train MAE: 0.7063105275430754\n",
      "Iteration: 74  train RMSE: 0.890490888139088 train MAE: 0.702207067493596\n",
      "Iteration: 75  train RMSE: 0.8900552501773477 train MAE: 0.7053234163082747\n"
     ]
    }
   ],
   "source": [
    "model_v=MatrixFactorization()\n",
    "model_v.fit(data_df)\n",
    "U = model_v.U\n",
    "M = model_v.M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75979f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('users_mf.csv', U, delimiter=',')\n",
    "np.savetxt('movies_mf.csv', M, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
